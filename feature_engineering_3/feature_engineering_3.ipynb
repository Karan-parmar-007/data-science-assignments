{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Feature Engineering-3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n",
    "\n",
    "Min-Max scaling is a feature scaling technique used to normalize the range of independent variables or features of data. The formula for Min-Max scaling is:\n",
    "\n",
    "\\[ X' = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} \\]\n",
    "\n",
    "where \\(X\\) is the original value, \\(X_{\\text{min}}\\) is the minimum value of the feature, \\(X_{\\text{max}}\\) is the maximum value of the feature, and \\(X'\\) is the scaled value.\n",
    "\n",
    "This scaling transforms the data to a fixed range, typically [0, 1], but it can be adjusted to any range.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Suppose you have a dataset with the following feature values:\n",
    "\n",
    "\\[ X = [10, 20, 30, 40, 50] \\]\n",
    "\n",
    "Using Min-Max scaling to transform the values to the range [0, 1]:\n",
    "\n",
    "1. Identify \\(X_{\\text{min}}\\) and \\(X_{\\text{max}}\\):\n",
    "   - \\(X_{\\text{min}} = 10\\)\n",
    "   - \\(X_{\\text{max}} = 50\\)\n",
    "\n",
    "2. Apply the Min-Max scaling formula:\n",
    "\n",
    "\\[ X' = \\frac{X - 10}{50 - 10} = \\frac{X - 10}{40} \\]\n",
    "\n",
    "So, the scaled values will be:\n",
    "\n",
    "\\[ X' = \\left[ \\frac{10-10}{40}, \\frac{20-10}{40}, \\frac{30-10}{40}, \\frac{40-10}{40}, \\frac{50-10}{40} \\right] = [0, 0.25, 0.5, 0.75, 1] \\]\n",
    "\n",
    "### Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.\n",
    "\n",
    "The Unit Vector technique in feature scaling transforms the feature vector to have a unit norm (i.e., the vector length is 1). This scaling is particularly useful when the direction of the data matters more than the magnitude.\n",
    "\n",
    "The formula for Unit Vector scaling (also called normalization) is:\n",
    "\n",
    "\\[ X' = \\frac{X}{||X||} \\]\n",
    "\n",
    "where \\(||X||\\) is the Euclidean norm (length) of the vector \\(X\\).\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Consider a feature vector:\n",
    "\n",
    "\\[ X = [3, 4] \\]\n",
    "\n",
    "The Euclidean norm \\(||X||\\) is calculated as:\n",
    "\n",
    "\\[ ||X|| = \\sqrt{3^2 + 4^2} = \\sqrt{9 + 16} = 5 \\]\n",
    "\n",
    "Applying the Unit Vector scaling:\n",
    "\n",
    "\\[ X' = \\frac{X}{5} = \\left[ \\frac{3}{5}, \\frac{4}{5} \\right] = [0.6, 0.8] \\]\n",
    "\n",
    "**Difference from Min-Max scaling:**\n",
    "\n",
    "- Min-Max scaling adjusts the range of the data to a specified interval (e.g., [0, 1]).\n",
    "- Unit Vector scaling normalizes the length of the feature vector to 1, keeping the direction the same but changing the magnitude.\n",
    "\n",
    "### Q3. What is PCA (Principal Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n",
    "\n",
    "Principal Component Analysis (PCA) is a statistical technique used to reduce the dimensionality of a dataset while retaining most of the variance (information). It does this by transforming the original features into a new set of orthogonal features called principal components. These principal components are linear combinations of the original features and are ordered by the amount of variance they explain in the data.\n",
    "\n",
    "**Steps in PCA:**\n",
    "\n",
    "1. **Standardize the Data**: Center the data by subtracting the mean of each feature.\n",
    "2. **Compute the Covariance Matrix**: Calculate the covariance matrix of the standardized data.\n",
    "3. **Eigen Decomposition**: Compute the eigenvalues and eigenvectors of the covariance matrix.\n",
    "4. **Sort Eigenvalues and Eigenvectors**: Sort the eigenvectors by their corresponding eigenvalues in descending order.\n",
    "5. **Select Principal Components**: Choose the top \\(k\\) eigenvectors to form a new feature space.\n",
    "6. **Transform Data**: Project the original data onto the new feature space.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Suppose you have a dataset with two features:\n",
    "\n",
    "\\[ X = \\left[ \\begin{array}{cc} 2 & 3 \\\\ 3 & 6 \\\\ 4 & 8 \\\\ 5 & 9 \\\\ \\end{array} \\right] \\]\n",
    "\n",
    "1. **Standardize the data** (subtract the mean of each column).\n",
    "2. **Compute the covariance matrix**:\n",
    "\n",
    "\\[ \\text{Cov}(X) = \\left[ \\begin{array}{cc} 1.6667 & 2.5 \\\\ 2.5 & 5 \\\\ \\end{array} \\right] \\]\n",
    "\n",
    "3. **Eigen Decomposition** to find eigenvalues and eigenvectors:\n",
    "\n",
    "\\[ \\text{Eigenvalues} = \\{6.116, 0.550\\} \\]\n",
    "\\[ \\text{Eigenvectors} = \\left[ \\begin{array}{cc} 0.447 & -0.894 \\\\ 0.894 & 0.447 \\\\ \\end{array} \\right] \\]\n",
    "\n",
    "4. **Select Principal Components**: Choose the eigenvector with the largest eigenvalue as the first principal component.\n",
    "\n",
    "5. **Transform Data**: Project the data onto the new principal component space.\n",
    "\n",
    "### Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n",
    "\n",
    "PCA is a technique for feature extraction that transforms the original features into a new set of features (principal components) that are orthogonal and ordered by the amount of variance they explain in the data. This process reduces the dimensionality of the data while retaining the most significant features.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Suppose you have a dataset with three features:\n",
    "\n",
    "\\[ X = \\left[ \\begin{array}{ccc} 2 & 4 & 6 \\\\ 3 & 6 & 9 \\\\ 4 & 8 & 12 \\\\ 5 & 10 & 15 \\\\ \\end{array} \\right] \\]\n",
    "\n",
    "Applying PCA:\n",
    "\n",
    "1. **Standardize the data**.\n",
    "2. **Compute the covariance matrix**:\n",
    "\n",
    "\\[ \\text{Cov}(X) = \\left[ \\begin{array}{ccc} 1.6667 & 3.3333 & 5 \\\\ 3.3333 & 6.6667 & 10 \\\\ 5 & 10 & 15 \\\\ \\end{array} \\right] \\]\n",
    "\n",
    "3. **Eigen Decomposition** to find eigenvalues and eigenvectors:\n",
    "\n",
    "\\[ \\text{Eigenvalues} = \\{22.5, 0, 0\\} \\]\n",
    "\\[ \\text{Eigenvectors} = \\left[ \\begin{array}{ccc} 0.2673 & -0.5345 & 0.8018 \\\\ 0.5345 & 0.8018 & -0.2673 \\\\ 0.8018 & -0.2673 & -0.5345 \\\\ \\end{array} \\right] \\]\n",
    "\n",
    "4. **Select Principal Components**: Choose the eigenvector with the largest eigenvalue as the first principal component.\n",
    "\n",
    "5. **Transform Data**: Project the data onto the new principal component space, resulting in a lower-dimensional representation of the original data.\n",
    "\n",
    "### Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.\n",
    "\n",
    "To use Min-Max scaling for preprocessing the data in the recommendation system:\n",
    "\n",
    "1. **Collect Data**: Gather the dataset with features such as price, rating, and delivery time.\n",
    "2. **Identify Min and Max Values**: For each feature, identify the minimum and maximum values.\n",
    "3. **Apply Min-Max Scaling**: Use the Min-Max scaling formula to transform each feature to the range [0, 1] (or another specified range).\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Suppose the dataset contains the following values:\n",
    "\n",
    "\\[ \\text{Price} = [10, 20, 30, 40, 50] \\]\n",
    "\\[ \\text{Rating} = [3, 4, 2, 5, 1] \\]\n",
    "\\[ \\text{Delivery Time} = [30, 45, 60, 90, 120] \\]\n",
    "\n",
    "For each feature, apply Min-Max scaling:\n",
    "\n",
    "- **Price**:\n",
    "  \\[ \\text{Price}_{\\text{min}} = 10 \\]\n",
    "  \\[ \\text{Price}_{\\text{max}} = 50 \\]\n",
    "  \\[ \\text{Price}' = \\frac{\\text{Price} - 10}{50 - 10} = \\frac{\\text{Price} - 10}{40} \\]\n",
    "  \\[ \\text{Price}' = [0, 0.25, 0.5, 0.75, 1] \\]\n",
    "\n",
    "- **Rating**:\n",
    "  \\[ \\text{Rating}_{\\text{min}} = 1 \\]\n",
    "  \\[ \\text{Rating}_{\\text{max}} = 5 \\]\n",
    "  \\[ \\text{Rating}' = \\frac{\\text{Rating} - 1}{5 - 1} = \\frac{\\text{Rating} - 1}{4} \\]\n",
    "  \\[ \\text{Rating}' = [0.5, 0.75, 0.25, 1, 0] \\]\n",
    "\n",
    "- **Delivery Time**:\n",
    "  \\[ \\text{Delivery Time}_{\\text{min}} = 30 \\]\n",
    "  \\[ \\text{Delivery Time}_{\\text{max}} = 120 \\]\n",
    "  \\[ \\text{Delivery Time}' = \\frac{\\text{Delivery Time} - 30}{120 - 30} = \\frac{\\text{Delivery Time} - 30}{90} \\]\n",
    "  \\[ \\text{Delivery Time}' = [0, 0.167, 0.333, 0.667, 1] \\]\n",
    "\n",
    "After scaling, the dataset would be:\n",
    "\n",
    "\\[\n",
    "\\begin{array}{ccc}\n",
    "\\text{Price} & \\text{Rating} & \\text{Delivery Time} \\\\\n",
    "0 & 0.5 & 0 \\\\\n",
    "0.25 & 0.75 & 0.167 \\\\\n",
    "0.5 & 0.25 & 0.333 \\\\\n",
    "0.75 & 1 & 0.667 \\\\\n",
    "1 & 0 & 1 \\\\\n",
    "\\end{array}\n",
    "\\]\n",
    "\n",
    "### Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n",
    "\n",
    "To use PCA for reducing the dimensionality of the dataset:\n",
    "\n",
    "1. **Standardize the Data**: Center the data by subtracting the mean of each feature and scaling to unit variance.\n",
    "2. **Compute the Covariance Matrix**: Calculate the covariance matrix of the standardized data to understand the variance relationships between features.\n",
    "3. **Eigen Decomposition**: Compute the eigenvalues and eigenvectors of the covariance matrix.\n",
    "4. **Sort Eigenvalues and Eigenvectors**: Sort the eigenvectors by their corresponding eigenvalues in descending order, as eigenvalues indicate the amount of variance explained by each principal component.\n",
    "5. **Select Principal Components**: Choose the top \\(k\\) eigenvectors (principal components) that account for a significant portion of the total variance (e.g., 95%).\n",
    "6. **Transform Data**: Project the original data onto the new principal component space to obtain a reduced-dimensional representation.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Suppose you have a dataset with 100 features:\n",
    "\n",
    "1. **Standardize the data**.\n",
    "2. **Compute the covariance matrix**:\n",
    "\n",
    "\\[ \\text{Cov}(X) = \\left[ \\begin{array}{ccc}\n",
    "\\sigma_1^2 & \\cdots & \\sigma_{1,100} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\sigma_{100,1} & \\cdots & \\sigma_{100}^2 \\\\\n",
    "\\end{array} \\right] \\]\n",
    "\n",
    "3. **Eigen Decomposition** to find eigenvalues and eigenvectors.\n",
    "4. **Sort Eigenvalues and Eigenvectors** and choose the top \\(k\\) principal components that explain 95% of the variance.\n",
    "5. **Transform Data** by projecting it onto the selected principal components.\n",
    "\n",
    "This reduces the dataset to a smaller number of features while retaining most of the important information, making it more manageable for modeling.\n",
    "\n",
    "### Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.\n",
    "\n",
    "To perform Min-Max scaling to transform the values to a range of -1 to 1:\n",
    "\n",
    "1. **Identify Min and Max Values**:\n",
    "   - \\(X_{\\text{min}} = 1\\)\n",
    "   - \\(X_{\\text{max}} = 20\\)\n",
    "\n",
    "2. **Apply Min-Max Scaling Formula** to transform to the range [-1, 1]:\n",
    "\n",
    "\\[ X' = \\frac{(X - X_{\\text{min}}) \\times (1 - (-1))}{X_{\\text{max}} - X_{\\text{min}}} + (-1) \\]\n",
    "\n",
    "\\[ X' = \\frac{(X - 1) \\times 2}{20 - 1} - 1 \\]\n",
    "\n",
    "\\[ X' = \\frac{(X - 1) \\times 2}{19} - 1 \\]\n",
    "\n",
    "3. **Transform Each Value**:\n",
    "\n",
    "\\[ X' = \\left[ \\frac{(1 - 1) \\times 2}{19} - 1, \\frac{(5 - 1) \\times 2}{19} - 1, \\frac{(10 - 1) \\times 2}{19} - 1, \\frac{(15 - 1) \\times 2}{19} - 1, \\frac{(20 - 1) \\times 2}{19} - 1 \\right] \\]\n",
    "\n",
    "\\[ X' = \\left[ -1, \\frac{4 \\times 2}{19} - 1, \\frac{9 \\times 2}{19} - 1, \\frac{14 \\times 2}{19} - 1, \\frac{19 \\times 2}{19} - 1 \\right] \\]\n",
    "\n",
    "\\[ X' = \\left[ -1, -0.579, -0.053, 0.474, 1 \\right] \\]\n",
    "\n",
    "### Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "\n",
    "To perform Feature Extraction using PCA on the dataset with features [height, weight, age, gender, blood pressure]:\n",
    "\n",
    "1. **Standardize the Data**: Center the data by subtracting the mean of each feature and scaling to unit variance.\n",
    "2. **Compute the Covariance Matrix**: Calculate the covariance matrix of the standardized data.\n",
    "3. **Eigen Decomposition**: Compute the eigenvalues and eigenvectors of the covariance matrix.\n",
    "4. **Sort Eigenvalues and Eigenvectors**: Sort the eigenvectors by their corresponding eigenvalues in descending order.\n",
    "5. **Select Principal Components**: Choose the top \\(k\\) principal components that explain a significant portion of the total variance.\n",
    "\n",
    "**Determining the Number of Principal Components to Retain:**\n",
    "\n",
    "- **Explained Variance**: Look at the cumulative explained variance ratio. Typically, you retain enough principal components to explain around 95% of the variance. This ensures that you capture most of the important information in the data while reducing dimensionality.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Suppose the explained variance ratios for the principal components are:\n",
    "\n",
    "- PC1: 50%\n",
    "- PC2: 30%\n",
    "- PC3: 10%\n",
    "- PC4: 5%\n",
    "- PC5: 5%\n",
    "\n",
    "Cumulative explained variance:\n",
    "\n",
    "- PC1: 50%\n",
    "- PC1 + PC2: 80%\n",
    "- PC1 + PC2 + PC3: 90%\n",
    "- PC1 + PC2 + PC3 + PC4: 95%\n",
    "- PC1 + PC2 + PC3 + PC4 + PC5: 100%\n",
    "\n",
    "Based on this, you would choose to retain the first four principal components because they explain 95% of the total variance. This balance ensures you maintain most of the data's information while reducing the number of features from 5 to 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Complete**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
