{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Forward & Backward Propagation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. What is the purpose of forward propagation in a neural network?\n",
    "Forward propagation is the process of passing input data through the layers of the neural network to produce an output. Its purpose is to compute the predicted output based on the input data and the current state of the network's weights and biases. This process is essential for making predictions and evaluating the performance of the network during both training and inference phases.\n",
    "\n",
    "### Q2. How is forward propagation implemented mathematically in a single-layer feedforward neural network?\n",
    "In a single-layer feedforward neural network (also known as a perceptron), forward propagation is implemented mathematically as follows:\n",
    "\n",
    "1. **Input Vector (\\(\\mathbf{x}\\)):** An input vector \\(\\mathbf{x} = [x_1, x_2, \\ldots, x_n]\\).\n",
    "2. **Weights (\\(\\mathbf{w}\\)):** A weight vector \\(\\mathbf{w} = [w_1, w_2, \\ldots, w_n]\\).\n",
    "3. **Bias (\\(b\\)):** A bias term \\(b\\).\n",
    "4. **Linear Combination:** Compute the weighted sum of inputs and bias: \\(z = \\mathbf{w} \\cdot \\mathbf{x} + b = \\sum_{i=1}^{n} w_i x_i + b\\).\n",
    "5. **Activation Function (\\(f\\)):** Apply an activation function to the linear combination to get the output: \\(a = f(z)\\).\n",
    "\n",
    "### Q3. How are activation functions used during forward propagation?\n",
    "Activation functions are used to introduce non-linearity into the neural network. After computing the linear combination of inputs, weights, and bias, the activation function is applied to this result. This non-linear transformation allows the network to learn and represent more complex patterns in the data. Common activation functions include the sigmoid, hyperbolic tangent (tanh), ReLU (Rectified Linear Unit), and softmax.\n",
    "\n",
    "### Q4. What is the role of weights and biases in forward propagation?\n",
    "Weights and biases are the trainable parameters of a neural network. \n",
    "\n",
    "- **Weights:** These parameters are applied to the input features to determine the importance of each feature. They are multiplied by the input values.\n",
    "- **Biases:** These parameters allow the model to shift the activation function, providing additional flexibility to fit the data better. They are added to the weighted sum of inputs.\n",
    "\n",
    "Together, weights and biases help the network to map input data to the desired output by learning from the data during training.\n",
    "\n",
    "### Q5. What is the purpose of applying a softmax function in the output layer during forward propagation?\n",
    "The softmax function is applied in the output layer of a neural network to convert the raw output scores (logits) into probabilities. This is particularly useful in multi-class classification problems. The softmax function ensures that the output probabilities sum up to 1, making it easier to interpret the output as a probability distribution over different classes. The formula for softmax for class \\(j\\) is given by:\n",
    "\n",
    "\\[ \\text{softmax}(z_j) = \\frac{e^{z_j}}{\\sum_{k=1}^{K} e^{z_k}} \\]\n",
    "\n",
    "where \\(z_j\\) is the raw output score for class \\(j\\) and \\(K\\) is the total number of classes.\n",
    "\n",
    "### Q6. What is the purpose of backward propagation in a neural network?\n",
    "Backward propagation (backpropagation) is the process of computing the gradient of the loss function with respect to each weight and bias in the network. The purpose of backpropagation is to update the weights and biases to minimize the loss function, thereby improving the network's performance. It involves propagating the error from the output layer back through the network layers, using the chain rule to compute the gradients.\n",
    "\n",
    "### Q7. How is backward propagation mathematically calculated in a single-layer feedforward neural network?\n",
    "In a single-layer feedforward neural network, backpropagation involves the following steps:\n",
    "\n",
    "1. **Compute the error (\\(e\\)):** Calculate the difference between the predicted output (\\(a\\)) and the actual target (\\(y\\)): \\(e = a - y\\).\n",
    "2. **Compute the gradient of the loss with respect to the output (\\(\\frac{\\partial L}{\\partial z}\\)):** Using the derivative of the loss function.\n",
    "3. **Compute the gradient of the loss with respect to the weights (\\(\\frac{\\partial L}{\\partial \\mathbf{w}}\\)):** \\(\\frac{\\partial L}{\\partial \\mathbf{w}} = e \\cdot \\mathbf{x}\\).\n",
    "4. **Compute the gradient of the loss with respect to the bias (\\(\\frac{\\partial L}{\\partial b}\\)):** \\(\\frac{\\partial L}{\\partial b} = e\\).\n",
    "5. **Update weights and biases:** Adjust the weights and biases using the gradients and a learning rate (\\(\\eta\\)): \\(\\mathbf{w} \\leftarrow \\mathbf{w} - \\eta \\cdot \\frac{\\partial L}{\\partial \\mathbf{w}}\\) and \\(b \\leftarrow b - \\eta \\cdot \\frac{\\partial L}{\\partial b}\\).\n",
    "\n",
    "### Q8. Can you explain the concept of the chain rule and its application in backward propagation?\n",
    "The chain rule is a fundamental principle in calculus used to compute the derivative of a composite function. In the context of backpropagation, the chain rule allows us to compute the gradient of the loss function with respect to each weight and bias in the network by breaking down the computation into simpler steps.\n",
    "\n",
    "For example, if we have a composite function \\(L(f(g(x)))\\), the chain rule states that the derivative of \\(L\\) with respect to \\(x\\) is:\n",
    "\n",
    "\\[ \\frac{dL}{dx} = \\frac{dL}{df} \\cdot \\frac{df}{dg} \\cdot \\frac{dg}{dx} \\]\n",
    "\n",
    "In backpropagation, this means that the gradient of the loss function with respect to the input of each layer can be computed by multiplying the gradient of the loss with respect to the output of that layer by the gradient of the output with respect to the input. This process is repeated backward through each layer of the network.\n",
    "\n",
    "### Q9. What are some common challenges or issues that can occur during backward propagation, and how can they be addressed?\n",
    "Some common challenges during backward propagation include:\n",
    "\n",
    "1. **Vanishing Gradients:** Gradients become very small, causing slow learning or stagnation. This can be addressed by using activation functions like ReLU, initializing weights properly, or using techniques like batch normalization.\n",
    "   \n",
    "2. **Exploding Gradients:** Gradients become very large, causing unstable learning. This can be addressed by gradient clipping, using a lower learning rate, or batch normalization.\n",
    "   \n",
    "3. **Overfitting:** The model performs well on training data but poorly on validation data. This can be addressed by using regularization techniques (e.g., L1/L2 regularization, dropout), increasing the amount of training data, or using data augmentation.\n",
    "   \n",
    "4. **Underfitting:** The model performs poorly on both training and validation data. This can be addressed by increasing model complexity, using more appropriate features, or reducing regularization.\n",
    "\n",
    "5. **Learning Rate Issues:** If the learning rate is too high, the model may fail to converge. If too low, the training process can be very slow. This can be addressed by using learning rate schedules or adaptive learning rate optimizers like Adam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **COMPLETE**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
