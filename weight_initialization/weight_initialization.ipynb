{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Weight Initialization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Understanding Weight Initialization\n",
    "\n",
    "#### _k: Explain the importance of weight initialization in artificial neural networks. Why is it necessary to initialize the weights carefully?\n",
    "\n",
    "Weight initialization is crucial in artificial neural networks because it significantly affects the convergence speed and the final performance of the model. Proper initialization helps:\n",
    "\n",
    "1. **Break Symmetry**: Ensures that neurons start with different weights, allowing them to learn different features.\n",
    "2. **Avoid Vanishing/Exploding Gradients**: Helps maintain gradients at a manageable scale during backpropagation, facilitating stable and efficient learning.\n",
    "3. **Accelerate Convergence**: Proper initialization can speed up the convergence of the optimization algorithm by starting the training process closer to the optimal solution.\n",
    "\n",
    "#### Bk: Describe the challenges associated with improper weight initialization. How do these issues affect model training and convergence?\n",
    "\n",
    "Improper weight initialization can lead to several issues:\n",
    "\n",
    "1. **Vanishing Gradients**: If weights are too small, gradients can become exceedingly small during backpropagation, causing the learning process to be extremely slow or even halt.\n",
    "2. **Exploding Gradients**: If weights are too large, gradients can grow exponentially during backpropagation, causing instability and divergence in the learning process.\n",
    "3. **Slow Convergence**: Improper initialization can lead to slow convergence, requiring more epochs to reach an optimal solution.\n",
    "4. **Suboptimal Solutions**: Can result in the model getting stuck in poor local minima, leading to suboptimal performance.\n",
    "\n",
    "#### >k: Discuss the concept of variance and how it relates to weight initialization. Why is it crucial to consider the variance of weights during initialization?\n",
    "\n",
    "Variance is a measure of the spread of the weight values. Considering the variance is crucial because:\n",
    "\n",
    "1. **Balanced Signal Propagation**: Proper variance ensures that the input signals propagate through the network without vanishing or exploding.\n",
    "2. **Stable Gradients**: Ensures that the gradients remain at a manageable scale during backpropagation, facilitating stable learning.\n",
    "3. **Efficient Learning**: Helps maintain the signal magnitude, enabling efficient learning and faster convergence.\n",
    "\n",
    "### Part 2: Weight Initialization Techniques\n",
    "\n",
    "#### ¤k: Explain the concept of zero initialization. Discuss its potential limitations and when it can be appropriate to use.\n",
    "\n",
    "Zero initialization involves setting all weights to zero. \n",
    "\n",
    "- **Potential Limitations**:\n",
    "  - **Symmetry Problem**: All neurons in a layer start with the same weights and learn the same features, leading to poor model performance.\n",
    "  - **No Learning**: Without any variance in initial weights, backpropagation can't update weights properly.\n",
    "\n",
    "- **When Appropriate**:\n",
    "  - It can be used to initialize biases, as biases are not subject to the symmetry problem.\n",
    "\n",
    "#### k: Describe the process of random initialization. How can random initialization be adjusted to mitigate potential issues like saturation or vanishing/exploding gradients?\n",
    "\n",
    "Random initialization involves setting weights to small random values drawn from a uniform or normal distribution.\n",
    "\n",
    "- **Mitigation Strategies**:\n",
    "  - **Normalized Initialization**: Adjusting the distribution by scaling it according to the number of inputs and outputs (e.g., He, Xavier initialization) to maintain variance.\n",
    "  - **Use of Bounds**: Ensuring weights are within a certain range to prevent extreme values that cause saturation or gradient issues.\n",
    "\n",
    "#### xk: Discuss the concept of Xavier/Glorot initialization. Explain how it addresses the challenges of improper weight initialization and the underlying theory behind it.\n",
    "\n",
    "Xavier/Glorot initialization sets weights to values drawn from a distribution with zero mean and a variance of \\( \\frac{2}{n_{in} + n_{out}} \\), where \\( n_{in} \\) is the number of input units and \\( n_{out} \\) is the number of output units.\n",
    "\n",
    "- **Addresses Challenges**:\n",
    "  - **Balanced Variance**: Ensures that the variance of inputs and outputs is preserved, preventing vanishing or exploding gradients.\n",
    "  - **Efficient Learning**: Facilitates faster convergence by maintaining stable gradient values.\n",
    "\n",
    "- **Theory**: Based on maintaining the variance of activations and gradients across layers to ensure stable and efficient training.\n",
    "\n",
    "#### k: Explain the concept of He initialization. How does it differ from Xavier initialization, and when is it preferred?\n",
    "\n",
    "He initialization sets weights to values drawn from a distribution with zero mean and a variance of \\( \\frac{2}{n_{in}} \\), where \\( n_{in} \\) is the number of input units.\n",
    "\n",
    "- **Differences**:\n",
    "  - **Higher Variance**: Uses \\( \\frac{2}{n_{in}} \\) instead of \\( \\frac{2}{n_{in} + n_{out}} \\), providing slightly larger weights.\n",
    "  - **Activation Functions**: He initialization is particularly effective for ReLU and its variants, which do not squash the output range as much as other activation functions.\n",
    "\n",
    "- **When Preferred**: Used for deep networks, especially those with ReLU activation functions, to maintain variance and prevent gradient issues.\n",
    "\n",
    "### Part 3: Applying Weight Initialization\n",
    "\n",
    "#### Êk: Implement different weight initialization techniques (zero initialization, random initialization, Xavier initialization, and He initialization) in a neural network using a framework of your choice. Train the model on a suitable dataset and compare the performance of the initialized models.\n",
    "\n",
    "Let's implement the weight initialization techniques using TensorFlow and train a simple neural network on the MNIST dataset.\n",
    "\n",
    "\n",
    "#### ¶k: Discuss the considerations and tradeoffs when choosing the appropriate weight initialization technique for a given neural network architecture and task.\n",
    "\n",
    "When choosing a weight initialization technique, consider the following:\n",
    "\n",
    "1. **Activation Functions**: Different activations (e.g., ReLU, tanh) may benefit from specific initializations (e.g., He for ReLU).\n",
    "2. **Network Depth**: Deep networks are more prone to gradient issues, making techniques like He or Xavier more appropriate.\n",
    "3. **Task Complexity**: More complex tasks may require more sophisticated initialization to ensure stable learning.\n",
    "4. **Computational Resources**: Some initializations may require more computational resources due to additional calculations (e.g., sampling from specific distributions).\n",
    "\n",
    "Tradeoffs include:\n",
    "\n",
    "- **Complexity vs. Performance**: More advanced initialization methods (e.g., He, Xavier) can improve performance but add complexity.\n",
    "- **Training Time vs. Convergence**: Proper initialization can reduce training time by speeding up convergence but might require more tuning.\n",
    "- **Generalization vs. Overfitting**: Improper initialization can lead to overfitting or poor generalization, while proper initialization helps achieve a balance.\n",
    "\n",
    "Choosing the right initialization technique requires understanding the specific requirements and challenges of the neural network architecture and the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-04 09:28:03.441697: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-04 09:28:03.476069: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-04 09:28:03.485937: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-04 09:28:03.517966: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-04 09:28:05.374260: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/karan/virtual_envs/tf/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1722743888.619590    9380 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1722743888.699848    9380 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1722743888.699967    9380 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1722743888.709346    9380 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1722743888.709461    9380 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1722743888.709520    9380 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1722743888.970736    9380 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1722743888.970853    9380 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-08-04 09:28:08.970869: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1722743888.970955    9380 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-08-04 09:28:08.970987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2024-08-04 09:28:09.517537: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 188160000 exceeds 10% of free system memory.\n",
      "2024-08-04 09:28:09.749683: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 188160000 exceeds 10% of free system memory.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1722743890.715134    9507 service.cc:146] XLA service 0x7fc69c008d90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1722743890.715260    9507 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "2024-08-04 09:28:10.755025: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-08-04 09:28:10.912562: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907\n",
      "I0000 00:00:1722743892.718873    9507 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2024-08-04 09:28:50.866918: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 188160000 exceeds 10% of free system memory.\n",
      "2024-08-04 09:28:51.137701: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 188160000 exceeds 10% of free system memory.\n",
      "2024-08-04 09:29:42.512830: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization: zeros\n",
      "Final training accuracy: 0.11236666887998581\n",
      "Final validation accuracy: 0.11349999904632568\n",
      "\n",
      "Initialization: random_normal\n",
      "Final training accuracy: 0.9858166575431824\n",
      "Final validation accuracy: 0.9742000102996826\n",
      "\n",
      "Initialization: xavier\n",
      "Final training accuracy: 0.9854666590690613\n",
      "Final validation accuracy: 0.9753000140190125\n",
      "\n",
      "Initialization: he\n",
      "Final training accuracy: 0.9858333468437195\n",
      "Final validation accuracy: 0.9775999784469604\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.initializers import Zeros, RandomNormal, GlorotNormal, HeNormal\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Define a simple model\n",
    "def create_model(initializer):\n",
    "    model = models.Sequential([\n",
    "        layers.Flatten(input_shape=(28, 28)),\n",
    "        layers.Dense(128, activation='relu', kernel_initializer=initializer),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Different initializers\n",
    "initializers = {\n",
    "    'zeros': Zeros(),\n",
    "    'random_normal': RandomNormal(mean=0.0, stddev=0.05),\n",
    "    'xavier': GlorotNormal(),\n",
    "    'he': HeNormal()\n",
    "}\n",
    "\n",
    "# Train and evaluate models with different initializations\n",
    "results = {}\n",
    "for name, initializer in initializers.items():\n",
    "    model = create_model(initializer)\n",
    "    history = model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test), verbose=0)\n",
    "    results[name] = history.history\n",
    "\n",
    "# Display results\n",
    "for name, history in results.items():\n",
    "    print(f\"Initialization: {name}\")\n",
    "    print(f\"Final training accuracy: {history['accuracy'][-1]}\")\n",
    "    print(f\"Final validation accuracy: {history['val_accuracy'][-1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
