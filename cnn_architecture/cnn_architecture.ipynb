{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cnn Architecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**bp: Describe the purpose and benefits of pooling in CNNs.**\n",
    "\n",
    "Pooling is a downsampling operation used in convolutional neural networks (CNNs) to reduce the dimensionality of feature maps. Its purpose and benefits include:\n",
    "\n",
    "1. **Dimensionality Reduction:** Pooling reduces the spatial size of the feature maps, which decreases the number of parameters and computations in the network, leading to faster training and reduced risk of overfitting.\n",
    "2. **Translation Invariance:** Pooling helps make the network more invariant to small translations of the input image, improving the model's robustness to variations in the input.\n",
    "3. **Feature Extraction:** By summarizing the features in a local neighborhood, pooling helps in extracting dominant features and discarding less relevant information, thus aiding in better feature extraction.\n",
    "\n",
    "**ip: Explain the difference between min pooling and max pooling.**\n",
    "\n",
    "- **Max Pooling:** This operation selects the maximum value from each local patch of the feature map. It highlights the most prominent features within each region.\n",
    "  \n",
    "  Example:\n",
    "  \\[\n",
    "  \\text{Input patch: } \\begin{bmatrix} 1 & 3 \\\\ 2 & 4 \\end{bmatrix}, \\text{ Max pooled value: } 4\n",
    "  \\]\n",
    "\n",
    "- **Min Pooling:** This operation selects the minimum value from each local patch of the feature map. It can be used to highlight the least prominent features within each region.\n",
    "  \n",
    "  Example:\n",
    "  \\[\n",
    "  \\text{Input patch: } \\begin{bmatrix} 1 & 3 \\\\ 2 & 4 \\end{bmatrix}, \\text{ Min pooled value: } 1\n",
    "  \\]\n",
    "\n",
    "### Padding in CNN\n",
    "\n",
    "**Wp: Discuss the concept of padding in CNN and its significance.**\n",
    "\n",
    "Padding involves adding extra pixels around the border of an input feature map before applying convolution operations. The significance of padding includes:\n",
    "\n",
    "1. **Control Output Size:** Padding helps in controlling the spatial dimensions of the output feature maps. Without padding, the size of the feature map decreases after each convolution operation.\n",
    "2. **Preserve Information:** By padding the input, information from the borders of the input image is preserved, allowing the convolutional layers to better utilize all parts of the input image.\n",
    "3. **Maintain Spatial Dimensions:** In cases where it is necessary to maintain the same spatial dimensions between the input and output (e.g., for symmetric architectures), padding is essential.\n",
    "\n",
    "**qp: Compare and contrast zero-padding and valid-padding in terms of their effects on the output feature map size.**\n",
    "\n",
    "- **Zero-padding (same-padding):** This involves adding zeros around the border of the input feature map. It allows the convolution to be applied such that the output feature map has the same spatial dimensions as the input. This is useful when the input and output need to be of the same size.\n",
    "  \n",
    "  Example: \n",
    "  If the input is \\(5 \\times 5\\) and we use a \\(3 \\times 3\\) filter with zero-padding, the output will also be \\(5 \\times 5\\).\n",
    "\n",
    "- **Valid-padding:** This involves no padding. Convolution is applied only where the filter fits completely inside the input feature map. As a result, the output feature map is smaller than the input.\n",
    "  \n",
    "  Example: \n",
    "  If the input is \\(5 \\times 5\\) and we use a \\(3 \\times 3\\) filter with valid-padding, the output will be \\(3 \\times 3\\).\n",
    "\n",
    "### LeNet-5 Architecture\n",
    "\n",
    "**bp: Provide a brief overview of the LeNet-5 architecture.**\n",
    "\n",
    "LeNet-5 is one of the pioneering convolutional neural network architectures designed for handwritten digit recognition. It consists of the following layers:\n",
    "\n",
    "1. **Input Layer:** Takes a \\(32 \\times 32\\) grayscale image as input.\n",
    "2. **C1 – Convolutional Layer:** 6 filters of size \\(5 \\times 5\\), producing six \\(28 \\times 28\\) feature maps.\n",
    "3. **S2 – Subsampling Layer:** Average pooling with a \\(2 \\times 2\\) filter and a stride of 2, producing six \\(14 \\times 14\\) feature maps.\n",
    "4. **C3 – Convolutional Layer:** 16 filters of size \\(5 \\times 5\\), producing sixteen \\(10 \\times 10\\) feature maps.\n",
    "5. **S4 – Subsampling Layer:** Average pooling with a \\(2 \\times 2\\) filter and a stride of 2, producing sixteen \\(5 \\times 5\\) feature maps.\n",
    "6. **C5 – Convolutional Layer:** 120 filters of size \\(5 \\times 5\\), producing a \\(1 \\times 1 \\times 120\\) output.\n",
    "7. **F6 – Fully Connected Layer:** 84 neurons.\n",
    "8. **Output Layer:** Fully connected layer with 10 neurons (for 10 classes).\n",
    "\n",
    "**ip: Describe the key components of LeNet-5 and their respective purposes.**\n",
    "\n",
    "1. **Convolutional Layers (C1, C3, C5):** Extract spatial features by applying convolutional filters to the input. Each layer detects different types of features (edges, textures, etc.).\n",
    "2. **Subsampling (Pooling) Layers (S2, S4):** Reduce the dimensionality of the feature maps, preserving the most important information and reducing computational complexity.\n",
    "3. **Fully Connected Layers (F6, Output):** Integrate the features extracted by the convolutional layers to make final predictions. The output layer provides the class probabilities.\n",
    "\n",
    "**Wp: Discuss the advantages and limitations of LeNet-5 in the context of image classification tasks.**\n",
    "\n",
    "- **Advantages:**\n",
    "  - Early example of a successful CNN architecture.\n",
    "  - Simple and efficient for small datasets like MNIST.\n",
    "  - Demonstrated the effectiveness of convolutional layers combined with pooling layers.\n",
    "\n",
    "- **Limitations:**\n",
    "  - Not scalable to larger and more complex datasets.\n",
    "  - Lacks depth and capacity compared to modern CNN architectures.\n",
    "  - Performance may not be competitive for high-resolution images or complex classification tasks.\n",
    "\n",
    "\n",
    "\n",
    "**bp: Present an overview of the AlexNet architecture.**\n",
    "\n",
    "AlexNet, designed by Alex Krizhevsky et al., is a deep convolutional neural network that won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012. The architecture consists of:\n",
    "\n",
    "1. **Input Layer:** Takes a \\(224 \\times 224\\) RGB image.\n",
    "2. **5 Convolutional Layers:** Extract spatial features with different sizes and strides.\n",
    "3. **Max Pooling Layers:** Reduce the dimensionality of the feature maps.\n",
    "4. **3 Fully Connected Layers:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karan/virtual_envs/tf/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1722777911.149297    4272 service.cc:146] XLA service 0x7fbd84005640 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1722777911.149397    4272 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "2024-08-04 18:55:11.203662: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-08-04 18:55:11.449514: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907\n",
      "2024-08-04 18:55:15.134602: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_610', 104 bytes spill stores, 104 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 41/938\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.3222 - loss: 2.0875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1722777918.506602    4272 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - accuracy: 0.8254 - loss: 0.5588 - val_accuracy: 0.9725 - val_loss: 0.0847\n",
      "Epoch 2/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9724 - loss: 0.0896 - val_accuracy: 0.9804 - val_loss: 0.0622\n",
      "Epoch 3/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9814 - loss: 0.0606 - val_accuracy: 0.9827 - val_loss: 0.0525\n",
      "Epoch 4/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9856 - loss: 0.0462 - val_accuracy: 0.9843 - val_loss: 0.0478\n",
      "Epoch 5/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9887 - loss: 0.0358 - val_accuracy: 0.9878 - val_loss: 0.0373\n",
      "Epoch 6/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9906 - loss: 0.0317 - val_accuracy: 0.9872 - val_loss: 0.0414\n",
      "Epoch 7/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9923 - loss: 0.0248 - val_accuracy: 0.9892 - val_loss: 0.0365\n",
      "Epoch 8/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9927 - loss: 0.0224 - val_accuracy: 0.9886 - val_loss: 0.0340\n",
      "Epoch 9/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9939 - loss: 0.0183 - val_accuracy: 0.9887 - val_loss: 0.0398\n",
      "Epoch 10/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9946 - loss: 0.0170 - val_accuracy: 0.9912 - val_loss: 0.0361\n",
      "313/313 - 2s - 6ms/step - accuracy: 0.9912 - loss: 0.0361\n",
      "Test accuracy: 0.9912\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Add an extra dimension to the images and convert to float32\n",
    "train_images = tf.expand_dims(train_images, axis=-1)\n",
    "test_images = tf.expand_dims(test_images, axis=-1)\n",
    "\n",
    "# Resize the images to 32x32\n",
    "train_images = tf.image.resize(train_images, [32, 32])\n",
    "test_images = tf.image.resize(test_images, [32, 32])\n",
    "\n",
    "# Normalize the images\n",
    "train_images = tf.cast(train_images, tf.float32) / 255.0\n",
    "test_images = tf.cast(test_images, tf.float32) / 255.0\n",
    "\n",
    "# One-hot encode the labels\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "# Define the LeNet-5 architecture\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(6, (5, 5), activation='relu', input_shape=(32, 32, 1)),\n",
    "    layers.AveragePooling2D((2, 2)),\n",
    "    layers.Conv2D(16, (5, 5), activation='relu'),\n",
    "    layers.AveragePooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(120, activation='relu'),\n",
    "    layers.Dense(84, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_images, train_labels, epochs=10, \n",
    "                    batch_size=64, \n",
    "                    validation_data=(test_images, test_labels))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print(f'Test accuracy: {test_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **COMPLETE**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
