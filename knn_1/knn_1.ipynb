{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **KNN-1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. What is the KNN algorithm?\n",
    "\n",
    "The K-Nearest Neighbors (KNN) algorithm is a simple, non-parametric, lazy learning algorithm used for classification and regression tasks. It works by finding the `k` closest training examples in the feature space to a new data point and makes predictions based on the majority class (for classification) or the average value (for regression) of these neighbors. The distance between data points is typically measured using metrics like Euclidean or Manhattan distance.\n",
    "\n",
    "### Q2. How do you choose the value of K in KNN?\n",
    "\n",
    "Choosing the value of `k` in KNN involves a trade-off between bias and variance:\n",
    "\n",
    "- **Small `k`**: Low bias, high variance. The model may capture noise in the training data.\n",
    "- **Large `k`**: High bias, low variance. The model becomes smoother but may miss subtle patterns.\n",
    "\n",
    "Common methods for choosing `k` include:\n",
    "\n",
    "1. **Cross-Validation**: Splitting the training data into multiple folds, using different values of `k`, and selecting the value that minimizes the cross-validation error.\n",
    "2. **Heuristics**: Starting with the square root of the number of training samples (âˆšn) and adjusting based on performance.\n",
    "3. **Domain Knowledge**: Leveraging insights from the specific problem domain to set an appropriate value of `k`.\n",
    "\n",
    "### Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "\n",
    "- **KNN Classifier**: Used for classification tasks. It predicts the class of a new data point based on the majority class among the `k` nearest neighbors.\n",
    "- **KNN Regressor**: Used for regression tasks. It predicts the continuous value of a new data point by averaging the values of the `k` nearest neighbors.\n",
    "\n",
    "### Q4. How do you measure the performance of KNN?\n",
    "\n",
    "Performance of KNN can be measured using different metrics depending on the task:\n",
    "\n",
    "- **Classification**:\n",
    "  - Accuracy: The proportion of correctly classified instances.\n",
    "  - Precision, Recall, F1-Score: Especially useful for imbalanced classes.\n",
    "  - Confusion Matrix: Provides a detailed breakdown of classification results.\n",
    "\n",
    "- **Regression**:\n",
    "  - Mean Absolute Error (MAE): The average absolute difference between predicted and actual values.\n",
    "  - Mean Squared Error (MSE): The average squared difference between predicted and actual values.\n",
    "  - R-squared: The proportion of variance in the dependent variable that is predictable from the independent variables.\n",
    "\n",
    "### Q5. What is the curse of dimensionality in KNN?\n",
    "\n",
    "The curse of dimensionality refers to the phenomenon where the performance of KNN (and other algorithms) degrades as the number of dimensions (features) increases. In high-dimensional spaces, data points tend to be equidistant from each other, making it hard to identify meaningful neighbors. This can lead to:\n",
    "\n",
    "- Increased computational cost due to the higher number of dimensions.\n",
    "- Reduced performance due to sparsity and noise.\n",
    "\n",
    "### Q6. How do you handle missing values in KNN?\n",
    "\n",
    "Handling missing values in KNN can be done using several methods:\n",
    "\n",
    "1. **Imputation**:\n",
    "   - Mean/Median Imputation: Replacing missing values with the mean or median of the feature.\n",
    "   - KNN Imputation: Using KNN to impute missing values based on the `k` nearest neighbors.\n",
    "   \n",
    "2. **Removing Missing Values**: If the number of missing values is small, removing those data points or features might be an option.\n",
    "\n",
    "3. **Using Algorithms that Handle Missing Values**: Some KNN implementations are designed to handle missing values directly.\n",
    "\n",
    "### Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n",
    "\n",
    "- **KNN Classifier**:\n",
    "  - Better suited for classification tasks where the output is a discrete label.\n",
    "  - Performance depends on the majority class of the nearest neighbors.\n",
    "  - Sensitive to the choice of `k`, distance metric, and feature scaling.\n",
    "\n",
    "- **KNN Regressor**:\n",
    "  - Better suited for regression tasks where the output is a continuous value.\n",
    "  - Performance depends on the average value of the nearest neighbors.\n",
    "  - Also sensitive to the choice of `k`, distance metric, and feature scaling.\n",
    "\n",
    "### Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n",
    "\n",
    "**Strengths**:\n",
    "- Simple to implement and understand.\n",
    "- No training phase, making it suitable for online learning.\n",
    "- Works well with small datasets and non-linear decision boundaries.\n",
    "\n",
    "**Weaknesses**:\n",
    "- Computationally expensive for large datasets.\n",
    "- Performance deteriorates with high-dimensional data (curse of dimensionality).\n",
    "- Sensitive to irrelevant or redundant features.\n",
    "- Choosing the right value of `k` can be challenging.\n",
    "\n",
    "**Addressing Weaknesses**:\n",
    "- **Dimensionality Reduction**: Techniques like PCA or feature selection to reduce the number of dimensions.\n",
    "- **Efficient Data Structures**: Using KD-trees or ball trees for faster neighbor searches.\n",
    "- **Feature Scaling**: Standardizing or normalizing features to ensure fair distance calculations.\n",
    "- **Hyperparameter Tuning**: Using cross-validation to find the optimal `k`.\n",
    "\n",
    "### Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "\n",
    "- **Euclidean Distance**: Measures the shortest straight-line distance between two points in Euclidean space. It is sensitive to the scale of the features and can be influenced by outliers.\n",
    "\n",
    "  \\[\n",
    "  d(p, q) = \\sqrt{\\sum_{i=1}^{n} (p_i - q_i)^2}\n",
    "  \\]\n",
    "\n",
    "- **Manhattan Distance**: Measures the distance between two points along the axes at right angles. It is less sensitive to outliers and can handle high-dimensional data better than Euclidean distance in some cases.\n",
    "\n",
    "  \\[\n",
    "  d(p, q) = \\sum_{i=1}^{n} |p_i - q_i|\n",
    "  \\]\n",
    "\n",
    "### Q10. What is the role of feature scaling in KNN?\n",
    "\n",
    "Feature scaling is crucial in KNN because the algorithm relies on distance calculations between data points. If features are on different scales, those with larger ranges will dominate the distance calculations, leading to biased predictions. Common methods for feature scaling include:\n",
    "\n",
    "- **Standardization**: Transforming features to have zero mean and unit variance.\n",
    "- **Normalization**: Scaling features to a range, typically [0, 1].\n",
    "\n",
    "Proper feature scaling ensures that each feature contributes equally to the distance computation, improving the performance and reliability of the KNN algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **COMPLETE**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
