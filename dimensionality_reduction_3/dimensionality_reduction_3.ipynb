{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dimensionality Reduction-3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. What are Eigenvalues and Eigenvectors? How are they related to the Eigen-Decomposition approach? Explain with an example.\n",
    "\n",
    "**Eigenvalues** and **eigenvectors** are fundamental concepts in linear algebra associated with square matrices. \n",
    "\n",
    "- **Eigenvectors**: For a given square matrix \\( A \\), an eigenvector \\( \\mathbf{v} \\) is a non-zero vector such that when \\( A \\) is multiplied by \\( \\mathbf{v} \\), the product is a scalar multiple of \\( \\mathbf{v} \\). Mathematically, \\( A\\mathbf{v} = \\lambda \\mathbf{v} \\), where \\( \\lambda \\) is the eigenvalue associated with the eigenvector \\( \\mathbf{v} \\).\n",
    "- **Eigenvalues**: The scalars \\( \\lambda \\) are eigenvalues of \\( A \\) if there exists a non-zero vector \\( \\mathbf{v} \\) (eigenvector) such that \\( A\\mathbf{v} = \\lambda \\mathbf{v} \\).\n",
    "\n",
    "**Eigen-Decomposition** is a matrix factorization technique where a matrix \\( A \\) is decomposed into a product of its eigenvectors and eigenvalues. If \\( A \\) is diagonalizable, it can be written as:\n",
    "\\[ A = V \\Lambda V^{-1} \\]\n",
    "where \\( V \\) is a matrix whose columns are the eigenvectors of \\( A \\), \\( \\Lambda \\) is a diagonal matrix whose diagonal elements are the eigenvalues of \\( A \\), and \\( V^{-1} \\) is the inverse of \\( V \\).\n",
    "\n",
    "**Example**:\n",
    "Consider matrix \\( A \\):\n",
    "\\[ A = \\begin{pmatrix} 4 & 1 \\\\ 2 & 3 \\end{pmatrix} \\]\n",
    "\n",
    "To find the eigenvalues, we solve the characteristic equation \\( \\det(A - \\lambda I) = 0 \\):\n",
    "\\[ \\det\\begin{pmatrix} 4 - \\lambda & 1 \\\\ 2 & 3 - \\lambda \\end{pmatrix} = 0 \\]\n",
    "\\[ (4 - \\lambda)(3 - \\lambda) - 2 \\cdot 1 = 0 \\]\n",
    "\\[ \\lambda^2 - 7\\lambda + 10 = 0 \\]\n",
    "\\[ (\\lambda - 5)(\\lambda - 2) = 0 \\]\n",
    "\n",
    "So, the eigenvalues are \\( \\lambda_1 = 5 \\) and \\( \\lambda_2 = 2 \\).\n",
    "\n",
    "To find the eigenvectors, we solve \\( (A - \\lambda I) \\mathbf{v} = 0 \\) for each eigenvalue. For \\( \\lambda = 5 \\):\n",
    "\\[ \\begin{pmatrix} 4 - 5 & 1 \\\\ 2 & 3 - 5 \\end{pmatrix} \\mathbf{v} = \\begin{pmatrix} -1 & 1 \\\\ 2 & -2 \\end{pmatrix} \\mathbf{v} = 0 \\]\n",
    "\\[ \\mathbf{v}_1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\]\n",
    "\n",
    "For \\( \\lambda = 2 \\):\n",
    "\\[ \\begin{pmatrix} 4 - 2 & 1 \\\\ 2 & 3 - 2 \\end{pmatrix} \\mathbf{v} = \\begin{pmatrix} 2 & 1 \\\\ 2 & 1 \\end{pmatrix} \\mathbf{v} = 0 \\]\n",
    "\\[ \\mathbf{v}_2 = \\begin{pmatrix} -1 \\\\ 2 \\end{pmatrix} \\]\n",
    "\n",
    "Thus, \\( V = \\begin{pmatrix} 1 & -1 \\\\ 1 & 2 \\end{pmatrix} \\) and \\( \\Lambda = \\begin{pmatrix} 5 & 0 \\\\ 0 & 2 \\end{pmatrix} \\), so:\n",
    "\\[ A = V \\Lambda V^{-1} \\]\n",
    "\n",
    "### Q2. What is eigen decomposition and what is its significance in linear algebra?\n",
    "\n",
    "**Eigen-Decomposition** is a matrix factorization technique where a square matrix \\( A \\) is decomposed into a product of its eigenvectors and eigenvalues. The significance of eigen-decomposition in linear algebra includes:\n",
    "\n",
    "- **Simplification of Matrix Operations**: Many matrix operations, such as computing powers of a matrix, become easier when the matrix is diagonalized.\n",
    "- **Insights into Matrix Properties**: Eigenvalues and eigenvectors provide insights into important properties of the matrix, such as its determinant, trace, and rank.\n",
    "- **Principal Component Analysis (PCA)**: In data science and machine learning, eigen-decomposition is used in PCA to reduce the dimensionality of data while preserving as much variance as possible.\n",
    "\n",
    "### Q3. What are the conditions that must be satisfied for a square matrix to be diagonalizable using the Eigen-Decomposition approach? Provide a brief proof to support your answer.\n",
    "\n",
    "A square matrix \\( A \\) is diagonalizable if and only if there are enough linearly independent eigenvectors to form a basis for the vector space. Specifically, \\( A \\) is diagonalizable if:\n",
    "\n",
    "1. \\( A \\) has \\( n \\) linearly independent eigenvectors (where \\( n \\) is the size of the matrix).\n",
    "2. The geometric multiplicity (number of linearly independent eigenvectors) of each eigenvalue equals its algebraic multiplicity (multiplicity in the characteristic equation).\n",
    "\n",
    "**Proof**:\n",
    "Let \\( A \\) be an \\( n \\times n \\) matrix with \\( n \\) linearly independent eigenvectors \\( \\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n \\). Form matrix \\( V \\) by taking these eigenvectors as columns:\n",
    "\\[ V = [\\mathbf{v}_1 \\; \\mathbf{v}_2 \\; \\ldots \\; \\mathbf{v}_n] \\]\n",
    "\n",
    "Then \\( AV = V\\Lambda \\), where \\( \\Lambda \\) is a diagonal matrix of eigenvalues. Since \\( V \\) is invertible (its columns are linearly independent), we can write:\n",
    "\\[ A = V\\Lambda V^{-1} \\]\n",
    "\n",
    "Thus, \\( A \\) is diagonalizable.\n",
    "\n",
    "### Q4. What is the significance of the spectral theorem in the context of the Eigen-Decomposition approach? How is it related to the diagonalizability of a matrix? Explain with an example.\n",
    "\n",
    "The **spectral theorem** states that any symmetric matrix \\( A \\) (real and square) can be diagonalized by an orthogonal matrix. Specifically, \\( A \\) can be written as:\n",
    "\\[ A = Q \\Lambda Q^T \\]\n",
    "where \\( Q \\) is an orthogonal matrix whose columns are the eigenvectors of \\( A \\), and \\( \\Lambda \\) is a diagonal matrix of eigenvalues.\n",
    "\n",
    "**Significance**:\n",
    "- **Orthogonal Diagonalization**: Ensures that the eigenvectors can be chosen to be orthonormal.\n",
    "- **Stability and Efficiency**: Orthogonal transformations are numerically stable and efficient.\n",
    "- **Applications**: Used in various applications like PCA, quantum mechanics, and vibration analysis.\n",
    "\n",
    "**Example**:\n",
    "Consider the symmetric matrix \\( A \\):\n",
    "\\[ A = \\begin{pmatrix} 4 & 1 \\\\ 1 & 3 \\end{pmatrix} \\]\n",
    "\n",
    "The eigenvalues and eigenvectors are found similarly as in Q1. The eigenvectors are orthogonal, and \\( A \\) can be diagonalized using an orthogonal matrix \\( Q \\) and a diagonal matrix \\( \\Lambda \\).\n",
    "\n",
    "### Q5. How do you find the eigenvalues of a matrix and what do they represent?\n",
    "\n",
    "To find the eigenvalues of a matrix \\( A \\), solve the characteristic equation \\( \\det(A - \\lambda I) = 0 \\). This equation results from setting the determinant of \\( A - \\lambda I \\) to zero, where \\( I \\) is the identity matrix.\n",
    "\n",
    "Eigenvalues represent:\n",
    "- **Scaling Factors**: How much the eigenvectors are scaled when the matrix is applied.\n",
    "- **Matrix Properties**: Reflect properties like stability, oscillatory modes, and growth rates in dynamic systems.\n",
    "\n",
    "### Q6. What are eigenvectors and how are they related to eigenvalues?\n",
    "\n",
    "Eigenvectors are non-zero vectors \\( \\mathbf{v} \\) that satisfy \\( A\\mathbf{v} = \\lambda \\mathbf{v} \\) for a given matrix \\( A \\) and scalar \\( \\lambda \\) (eigenvalue). The relationship is that each eigenvector is associated with a corresponding eigenvalue, which scales the eigenvector when \\( A \\) is applied to it.\n",
    "\n",
    "### Q7. Can you explain the geometric interpretation of eigenvectors and eigenvalues?\n",
    "\n",
    "Geometrically:\n",
    "- **Eigenvectors**: Directions in the vector space that remain invariant (except for scaling) when the matrix transformation is applied.\n",
    "- **Eigenvalues**: The scaling factors by which the eigenvectors are stretched or compressed.\n",
    "\n",
    "For example, in a 2D space, if a transformation matrix stretches vectors along one axis and compresses them along another, the eigenvectors point along these axes, and the eigenvalues indicate the stretch or compression factors.\n",
    "\n",
    "### Q8. What are some real-world applications of eigen decomposition?\n",
    "\n",
    "Real-world applications include:\n",
    "- **Principal Component Analysis (PCA)**: Dimensionality reduction technique in data science.\n",
    "- **Stability Analysis**: In control systems and dynamical systems.\n",
    "- **Quantum Mechanics**: Solving the Schr√∂dinger equation to find energy levels.\n",
    "- **Image Compression**: Using eigenvalues and eigenvectors in algorithms like JPEG.\n",
    "\n",
    "### Q9. Can a matrix have more than one set of eigenvectors and eigenvalues?\n",
    "\n",
    "Yes, a matrix can have multiple sets of eigenvectors and eigenvalues, but this needs to be clarified based on the following scenarios:\n",
    "\n",
    "1. **Distinct Eigenvalues**: If a matrix has \\( n \\) distinct eigenvalues, it will have \\( n \\) linearly independent eigenvectors, forming a unique set of eigenvector-eigenvalue pairs.\n",
    "\n",
    "2. **Repeated Eigenvalues**: If a matrix has repeated eigenvalues, it may have multiple linearly independent eigenvectors associated with a single eigenvalue. This leads to the concept of eigenvalue multiplicity:\n",
    "   - **Algebraic Multiplicity**: The number of times an eigenvalue appears as a root of the characteristic polynomial.\n",
    "   - **Geometric Multiplicity**: The number of linearly independent eigenvectors associated with an eigenvalue.\n",
    "\n",
    "For example, a matrix may have an eigenvalue with an algebraic multiplicity of 2 but only one linearly independent eigenvector (geometric multiplicity of 1), in which case the matrix is not diagonalizable. Conversely, if the geometric multiplicity matches the algebraic multiplicity, the matrix can be diagonalized.\n",
    "\n",
    "# Q10. In what ways is the Eigen-Decomposition approach useful in data analysis and machine learning? Discuss at least three specific applications or techniques that rely on Eigen-Decomposition.\n",
    "\n",
    "Eigen-Decomposition is a powerful tool in data analysis and machine learning due to its ability to simplify complex matrices, reveal underlying patterns, and provide insights into the structure of data. Here are three specific applications and techniques that rely on Eigen-Decomposition:\n",
    "\n",
    "1. Principal Component Analysis (PCA): PCA is a dimensionality reduction technique used to reduce the number of features in a dataset while retaining as much variance as possible. Eigen-Decomposition is used to compute the principal components, which are the eigenvectors of the covariance matrix of the data. The top k eigenvectors (principal components) are selected to represent the data in a lower-dimensional space, effectively reducing the dimensionality while preserving the most important information.\n",
    "\n",
    "2. Spectral Clustering: Spectral clustering is a technique used to partition data points into different clusters based on their similarity. Eigen-Decomposition is used to compute the eigenvectors of the Laplacian matrix of the data, which captures the connectivity between data points. The eigenvectors corresponding to the smallest non-zero eigenvalues are used to represent the data in a lower-dimensional space, and then clustering algorithms like K-means or hierarchical clustering are applied to this reduced representation.\n",
    "\n",
    "3. Image Compression: Eigen-Decomposition can be used to compress images by representing them as a linear combination of basis vectors (eigenvectors) obtained from the eigen-decomposition of the covariance matrix of the image pixels. The top k eigenvectors (basis vectors) are selected to represent the image, and the coefficients of these basis vectors are stored instead of the original pixel values. This approach reduces the storage and computational requirements while maintaining a reasonable level of image quality.\n",
    "\n",
    "In summary, Eigen-Decomposition is a versatile and powerful tool in data analysis and machine learning, enabling various applications such as dimensionality reduction, clustering, and image compression. Its ability to simplify complex matrices and reveal underlying patterns makes it a valuable tool for analyzing and understanding complex datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **COMPLETE**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
