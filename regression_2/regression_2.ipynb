{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Regression 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?\n",
    "R-squared (R²) is a statistical measure that represents the proportion of the variance in the dependent variable that is predictable from the independent variable(s) in a linear regression model. It is calculated as:\n",
    "\n",
    "𝑅\n",
    "2\n",
    "=\n",
    "1\n",
    "−\n",
    "𝑆\n",
    "𝑆\n",
    "𝑟\n",
    "𝑒\n",
    "𝑠\n",
    "𝑆\n",
    "𝑆\n",
    "𝑡\n",
    "𝑜\n",
    "𝑡\n",
    "R \n",
    "2\n",
    " =1− \n",
    "SS \n",
    "tot\n",
    "​\n",
    " \n",
    "SS \n",
    "res\n",
    "​\n",
    " \n",
    "​\n",
    " \n",
    "\n",
    "Where:\n",
    "\n",
    "𝑆\n",
    "𝑆\n",
    "𝑟\n",
    "𝑒\n",
    "𝑠\n",
    "SS \n",
    "res\n",
    "​\n",
    "  is the sum of squares of residuals (also known as the sum of squared errors),\n",
    "𝑆\n",
    "𝑆\n",
    "𝑡\n",
    "𝑜\n",
    "𝑡\n",
    "SS \n",
    "tot\n",
    "​\n",
    "  is the total sum of squares (the sum of squares of the differences between the actual values and the mean of the dependent variable).\n",
    "What it represents:\n",
    "\n",
    "An \n",
    "𝑅\n",
    "2\n",
    "R \n",
    "2\n",
    "  value of 0 indicates that the model explains none of the variability of the response data around its mean.\n",
    "An \n",
    "𝑅\n",
    "2\n",
    "R \n",
    "2\n",
    "  value of 1 indicates that the model explains all the variability of the response data around its mean.\n",
    "Values between 0 and 1 indicate the extent to which the model explains the variability.\n",
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n",
    "Adjusted R-squared is a modified version of R-squared that adjusts for the number of predictors in the model. It is used to account for the addition of variables that do not improve the model and provides a more accurate measure of model fit when multiple predictors are involved.\n",
    "\n",
    "The formula for adjusted R-squared is:\n",
    "\n",
    "Adjusted \n",
    "𝑅\n",
    "2\n",
    "=\n",
    "1\n",
    "−\n",
    "(\n",
    "(\n",
    "1\n",
    "−\n",
    "𝑅\n",
    "2\n",
    ")\n",
    "(\n",
    "𝑛\n",
    "−\n",
    "1\n",
    ")\n",
    "𝑛\n",
    "−\n",
    "𝑘\n",
    "−\n",
    "1\n",
    ")\n",
    "Adjusted R \n",
    "2\n",
    " =1−( \n",
    "n−k−1\n",
    "(1−R \n",
    "2\n",
    " )(n−1)\n",
    "​\n",
    " )\n",
    "\n",
    "Where:\n",
    "\n",
    "𝑛\n",
    "n is the number of observations,\n",
    "𝑘\n",
    "k is the number of predictors.\n",
    "Difference from regular R-squared:\n",
    "\n",
    "Regular R-squared always increases as more predictors are added to the model, regardless of their relevance.\n",
    "Adjusted R-squared can decrease if the added predictors do not improve the model, providing a more accurate reflection of model performance.\n",
    "Q3. When is it more appropriate to use adjusted R-squared?\n",
    "Adjusted R-squared is more appropriate to use when comparing the goodness-of-fit of regression models that have a different number of predictors. It accounts for the model complexity and penalizes the addition of irrelevant predictors, thus providing a more reliable measure for model comparison.\n",
    "\n",
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?\n",
    "RMSE (Root Mean Squared Error), MSE (Mean Squared Error), and MAE (Mean Absolute Error) are common metrics used to evaluate the performance of regression models.\n",
    "\n",
    "MSE (Mean Squared Error):\n",
    "MSE\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "∑\n",
    "𝑖\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "(\n",
    "𝑦\n",
    "𝑖\n",
    "−\n",
    "𝑦\n",
    "^\n",
    "𝑖\n",
    ")\n",
    "2\n",
    "MSE= \n",
    "n\n",
    "1\n",
    "​\n",
    " ∑ \n",
    "i=1\n",
    "n\n",
    "​\n",
    " (y \n",
    "i\n",
    "​\n",
    " − \n",
    "y\n",
    "^\n",
    "​\n",
    "  \n",
    "i\n",
    "​\n",
    " ) \n",
    "2\n",
    " \n",
    "Represents the average of the squares of the errors between the actual and predicted values.\n",
    "\n",
    "RMSE (Root Mean Squared Error):\n",
    "RMSE\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "∑\n",
    "𝑖\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "(\n",
    "𝑦\n",
    "𝑖\n",
    "−\n",
    "𝑦\n",
    "^\n",
    "𝑖\n",
    ")\n",
    "2\n",
    "RMSE= \n",
    "n\n",
    "1\n",
    "​\n",
    " ∑ \n",
    "i=1\n",
    "n\n",
    "​\n",
    " (y \n",
    "i\n",
    "​\n",
    " − \n",
    "y\n",
    "^\n",
    "​\n",
    "  \n",
    "i\n",
    "​\n",
    " ) \n",
    "2\n",
    " \n",
    "​\n",
    " \n",
    "It is the square root of the MSE and provides a measure of the average magnitude of the error. It is in the same units as the target variable.\n",
    "\n",
    "MAE (Mean Absolute Error):\n",
    "MAE\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "∑\n",
    "𝑖\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "∣\n",
    "𝑦\n",
    "𝑖\n",
    "−\n",
    "𝑦\n",
    "^\n",
    "𝑖\n",
    "∣\n",
    "MAE= \n",
    "n\n",
    "1\n",
    "​\n",
    " ∑ \n",
    "i=1\n",
    "n\n",
    "​\n",
    " ∣y \n",
    "i\n",
    "​\n",
    " − \n",
    "y\n",
    "^\n",
    "​\n",
    "  \n",
    "i\n",
    "​\n",
    " ∣\n",
    "Represents the average of the absolute differences between the actual and predicted values.\n",
    "\n",
    "What they represent:\n",
    "\n",
    "RMSE and MSE give more weight to larger errors, which can be useful if large errors are particularly undesirable.\n",
    "MAE provides a linear score, treating all errors equally regardless of their magnitude.\n",
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis.\n",
    "Advantages:\n",
    "\n",
    "MSE:\n",
    "\n",
    "Penalizes larger errors more than smaller ones, making it useful when large errors are particularly problematic.\n",
    "Provides a smooth and differentiable loss function, useful for optimization in model training.\n",
    "RMSE:\n",
    "\n",
    "Similar to MSE, but more interpretable due to being in the same units as the target variable.\n",
    "Penalizes large errors, making it sensitive to outliers.\n",
    "MAE:\n",
    "\n",
    "Less sensitive to outliers compared to MSE and RMSE.\n",
    "Provides a linear error measure that is easy to interpret.\n",
    "Disadvantages:\n",
    "\n",
    "MSE:\n",
    "\n",
    "Can be heavily influenced by outliers due to squaring the errors.\n",
    "Less interpretable because it is not in the same units as the target variable.\n",
    "RMSE:\n",
    "\n",
    "Also sensitive to outliers.\n",
    "Can be less interpretable in comparison to MAE because it emphasizes larger errors.\n",
    "MAE:\n",
    "\n",
    "Does not penalize large errors as heavily, which might be a downside if large errors are particularly problematic.\n",
    "Less commonly used in optimization algorithms because it is not differentiable at zero.\n",
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?\n",
    "Lasso (Least Absolute Shrinkage and Selection Operator) regularization adds a penalty equal to the absolute value of the magnitude of coefficients to the loss function. The Lasso regression objective function is:\n",
    "\n",
    "Minimize\n",
    "(\n",
    "∑\n",
    "𝑖\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "(\n",
    "𝑦\n",
    "𝑖\n",
    "−\n",
    "𝑦\n",
    "^\n",
    "𝑖\n",
    ")\n",
    "2\n",
    "+\n",
    "𝜆\n",
    "∑\n",
    "𝑗\n",
    "=\n",
    "1\n",
    "𝑝\n",
    "∣\n",
    "𝑤\n",
    "𝑗\n",
    "∣\n",
    ")\n",
    "Minimize(∑ \n",
    "i=1\n",
    "n\n",
    "​\n",
    " (y \n",
    "i\n",
    "​\n",
    " − \n",
    "y\n",
    "^\n",
    "​\n",
    "  \n",
    "i\n",
    "​\n",
    " ) \n",
    "2\n",
    " +λ∑ \n",
    "j=1\n",
    "p\n",
    "​\n",
    " ∣w \n",
    "j\n",
    "​\n",
    " ∣)\n",
    "\n",
    "Where \n",
    "𝜆\n",
    "λ is the regularization parameter.\n",
    "\n",
    "Differences from Ridge regularization:\n",
    "\n",
    "Ridge regularization adds a penalty equal to the square of the magnitude of coefficients:\n",
    "Minimize\n",
    "(\n",
    "∑\n",
    "𝑖\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "(\n",
    "𝑦\n",
    "𝑖\n",
    "−\n",
    "𝑦\n",
    "^\n",
    "𝑖\n",
    ")\n",
    "2\n",
    "+\n",
    "𝜆\n",
    "∑\n",
    "𝑗\n",
    "=\n",
    "1\n",
    "𝑝\n",
    "𝑤\n",
    "𝑗\n",
    "2\n",
    ")\n",
    "Minimize(∑ \n",
    "i=1\n",
    "n\n",
    "​\n",
    " (y \n",
    "i\n",
    "​\n",
    " − \n",
    "y\n",
    "^\n",
    "​\n",
    "  \n",
    "i\n",
    "​\n",
    " ) \n",
    "2\n",
    " +λ∑ \n",
    "j=1\n",
    "p\n",
    "​\n",
    " w \n",
    "j\n",
    "2\n",
    "​\n",
    " )\n",
    "Lasso can shrink some coefficients to exactly zero, effectively performing feature selection.\n",
    "Ridge tends to shrink coefficients towards zero but never exactly zero, meaning it keeps all features.\n",
    "When to use Lasso:\n",
    "\n",
    "When feature selection is desired or when dealing with a large number of features, as it can automatically remove irrelevant features.\n",
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate.\n",
    "Regularized linear models add a penalty to the loss function for having large coefficients, discouraging complex models that overfit the training data. By shrinking the coefficients, they reduce the model's variance and make it less sensitive to noise in the training data.\n",
    "\n",
    "Example:\n",
    "Consider a dataset with many features where a simple linear regression model overfits the data, capturing noise as if it were a genuine pattern. By applying Lasso or Ridge regularization, the model is constrained to have smaller coefficients, simplifying the model and improving its generalization to unseen data.\n",
    "\n",
    "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis.\n",
    "Limitations of regularized linear models:\n",
    "\n",
    "Bias-variance tradeoff: Regularization introduces bias into the model, which can sometimes oversimplify the model, leading to underfitting.\n",
    "Interpretability: The addition of regularization terms can make the interpretation of coefficients less straightforward.\n",
    "Non-linearity: Regularized linear models assume a linear relationship between predictors and the response. They may perform poorly when the true relationship is non-linear, unless combined with non-linear transformations of the predictors.\n",
    "Feature engineering: In some cases, manual feature engineering or using more complex models (e.g., tree-based methods) might be more effective.\n",
    "\n",
    "### Q9. Comparing Model Performance Using RMSE and MAE\n",
    "\n",
    "When comparing the performance of two regression models using RMSE and MAE, it’s important to understand what these metrics represent and their respective advantages.\n",
    "\n",
    "- **RMSE (Root Mean Squared Error)**: This metric is sensitive to outliers because it squares the differences between predicted and actual values before averaging them. Thus, larger errors have a disproportionately high impact on the RMSE. It gives more weight to larger errors and is useful when large errors are particularly undesirable.\n",
    "\n",
    "- **MAE (Mean Absolute Error)**: This metric takes the absolute differences between predicted and actual values and averages them. It is less sensitive to outliers compared to RMSE, giving a linear score for each error.\n",
    "\n",
    "Given the values:\n",
    "- Model A: RMSE = 10\n",
    "- Model B: MAE = 8\n",
    "\n",
    "To determine which model is better, consider the context and the distribution of the errors:\n",
    "\n",
    "- If large errors are particularly undesirable, **Model A** with RMSE = 10 might indicate better performance despite a higher RMSE, as MAE does not emphasize larger errors.\n",
    "- However, if the model performance is generally good and outliers are not a significant concern, **Model B** with MAE = 8 may be preferable due to its lower average error.\n",
    "\n",
    "**Limitations**:\n",
    "- Comparing RMSE of one model with the MAE of another is not straightforward because they measure error differently.\n",
    "- The choice of metric should depend on the specific application and the cost of errors in that context.\n",
    "\n",
    "### Q10. Comparing Regularized Models Using Ridge and Lasso\n",
    "\n",
    "When comparing regularized linear models, the choice between Ridge and Lasso regularization depends on the specific characteristics of the dataset and the goals of the modeling.\n",
    "\n",
    "- **Ridge Regularization (L2)**: Adds a penalty equal to the sum of the squared coefficients. It shrinks all coefficients towards zero but never completely removes any.\n",
    "\n",
    "- **Lasso Regularization (L1)**: Adds a penalty equal to the absolute sum of the coefficients. It can shrink some coefficients to zero, effectively performing variable selection.\n",
    "\n",
    "Given the models:\n",
    "- Model A: Ridge regularization with a regularization parameter of 0.1\n",
    "- Model B: Lasso regularization with a regularization parameter of 0.5\n",
    "\n",
    "To determine which model is better:\n",
    "- **Model A** (Ridge with α = 0.1) might be preferable if you want to retain all features and prevent overfitting by reducing the complexity of the model without eliminating any features.\n",
    "- **Model B** (Lasso with α = 0.5) might be better if you suspect that many features are irrelevant or redundant, as Lasso can eliminate them, leading to a more interpretable and potentially simpler model.\n",
    "\n",
    "**Trade-offs and Limitations**:\n",
    "- **Model A**: Ridge regularization will retain all features, which might not be ideal if some features are irrelevant or have high collinearity.\n",
    "- **Model B**: Lasso regularization might perform better in terms of interpretability and sparsity but can be more sensitive to the choice of the regularization parameter and might eliminate relevant features if the parameter is too high.\n",
    "\n",
    "In summary, the choice between these models should be guided by the specific problem requirements, such as the need for feature selection (use Lasso) or handling multicollinearity without feature elimination (use Ridge). Regularization parameters also play a crucial role and should be tuned appropriately using cross-validation or other techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **COMPLETE**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
