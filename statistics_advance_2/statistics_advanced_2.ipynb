{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Statistics Advanced 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q1: What are the Probability Mass Function (PMF) and Probability Density Function (PDF)? Explain with an example.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Probability Mass Function (PMF) and Probability Density Function (PDF) are both tools used in probability theory and statistics to describe the distribution of a random variable, but they are used in different contexts depending on whether the random variable is discrete or continuous.\n",
    "\n",
    "1. **Probability Mass Function (PMF)**:\n",
    "   - The PMF is used for discrete random variables, which take on a countable number of distinct values.\n",
    "   - It gives the probability that a discrete random variable is exactly equal to some value.\n",
    "   - The sum of all probabilities in the PMF equals 1.\n",
    "   - The PMF is often represented as a function that maps each possible value of the random variable to its probability.\n",
    "   \n",
    "   **Example**: Consider rolling a fair six-sided die. The PMF for this scenario would be:\n",
    "\n",
    "   \\[\n",
    "   P(X = x) =\n",
    "   \\begin{cases}\n",
    "   \\frac{1}{6}, & \\text{if } x = 1, 2, 3, 4, 5, \\text{ or } 6 \\\\\n",
    "   0, & \\text{otherwise}\n",
    "   \\end{cases}\n",
    "   \\]\n",
    "\n",
    "   This PMF tells us that the probability of rolling any particular number (1, 2, 3, 4, 5, or 6) is \\( \\frac{1}{6} \\), and the probability of rolling any other number is 0.\n",
    "\n",
    "2. **Probability Density Function (PDF)**:\n",
    "   - The PDF is used for continuous random variables, which can take on any value within a given range.\n",
    "   - Unlike the PMF, the PDF itself does not directly give probabilities but rather gives the density of probabilities over a range of values.\n",
    "   - The area under the curve of the PDF over a certain interval gives the probability that the random variable falls within that interval.\n",
    "   - The integral of the PDF over its entire range equals 1.\n",
    "\n",
    "   **Example**: Consider the height of adult males in a population, assuming it follows a normal distribution. The PDF for this scenario would be a bell-shaped curve, described by the normal distribution equation:\n",
    "\n",
    "   \\[\n",
    "   f(x | \\mu, \\sigma^2) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n",
    "   \\]\n",
    "\n",
    "   Where \\( x \\) is the height, \\( \\mu \\) is the mean height, and \\( \\sigma^2 \\) is the variance of height in the population. This PDF describes the probability density of different heights; for example, it tells us the likelihood of finding a person with a height near the mean compared to someone significantly taller or shorter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q2: What is Cumulative Density Function (CDF)? Explain with an example. Why CDF is used?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cumulative Density Function (CDF) is a concept closely related to both the Probability Mass Function (PMF) and Probability Density Function (PDF). It's a function used in probability theory and statistics to describe the probability that a random variable takes on a value less than or equal to a given value.\n",
    "\n",
    "### Definition:\n",
    "For a random variable \\( X \\), the CDF, denoted as \\( F(x) \\), is defined as:\n",
    "\n",
    "- For discrete random variables (PMF): \n",
    "  \\[\n",
    "  F(x) = P(X \\leq x)\n",
    "  \\]\n",
    "  \n",
    "- For continuous random variables (PDF): \n",
    "  \\[\n",
    "  F(x) = \\int_{-\\infty}^{x} f(t) dt\n",
    "  \\]\n",
    "  Where \\( f(t) \\) is the PDF of \\( X \\).\n",
    "\n",
    "### Example:\n",
    "Let's consider the example of rolling a fair six-sided die. We already have the PMF for this scenario. Now, we can derive its CDF:\n",
    "\n",
    "\\[\n",
    "F(x) = P(X \\leq x) =\n",
    "\\begin{cases}\n",
    "0, & \\text{if } x < 1 \\\\\n",
    "\\frac{1}{6}, & \\text{if } 1 \\leq x < 2 \\\\\n",
    "\\frac{2}{6}, & \\text{if } 2 \\leq x < 3 \\\\\n",
    "\\frac{3}{6}, & \\text{if } 3 \\leq x < 4 \\\\\n",
    "\\frac{4}{6}, & \\text{if } 4 \\leq x < 5 \\\\\n",
    "\\frac{5}{6}, & \\text{if } 5 \\leq x < 6 \\\\\n",
    "1, & \\text{if } x \\geq 6\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "This CDF tells us the probability of rolling a number less than or equal to \\( x \\) for any value of \\( x \\) between 1 and 6. For example, \\( F(3) = \\frac{3}{6} \\) indicates that the probability of rolling a number less than or equal to 3 is \\( \\frac{3}{6} \\), which is equivalent to \\( \\frac{1}{2} \\).\n",
    "\n",
    "### Why CDF is used?\n",
    "1. **Understanding Distribution**: The CDF provides a comprehensive view of the distribution of a random variable. It shows how the probability accumulates as the value of the random variable increases.\n",
    "2. **Calculating Probabilities**: It facilitates calculating probabilities for a random variable falling within a certain range, as it sums up probabilities up to that point.\n",
    "3. **Comparison**: CDFs allow for easy comparison between different distributions or different parameters of the same distribution.\n",
    "4. **Statistical Analysis**: In statistical analysis, CDFs are used for hypothesis testing, confidence interval estimation, and other inferential procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q3: What are some examples of situations where the normal distribution might be used as a model? Explain how the parameters of the normal distribution relate to the shape of the distribution.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normal distribution, also known as the Gaussian distribution, is one of the most widely used probability distributions in various fields due to its versatility and applicability to a wide range of phenomena. Here are some examples of situations where the normal distribution might be used as a model:\n",
    "\n",
    "1. **Height of Individuals**: The heights of adult humans often follow a normal distribution, with most people clustered around the average height and fewer individuals at the extremes (very tall or very short).\n",
    "\n",
    "2. **IQ Scores**: Intelligence quotient (IQ) scores in a population tend to approximate a normal distribution, with the majority of individuals clustered around the average IQ score and fewer individuals at the extremes of high or low IQ.\n",
    "\n",
    "3. **Measurement Errors**: Measurement errors in experiments or observations often follow a normal distribution, assuming random errors are present.\n",
    "\n",
    "4. **Financial Data**: Returns on investments, such as stock prices, often follow a normal distribution under certain conditions, especially in the long run and when considering large portfolios.\n",
    "\n",
    "5. **Biological Phenomena**: Biological variables like birth weights, blood pressure, and enzyme activity levels in biological systems can sometimes be modeled using the normal distribution.\n",
    "\n",
    "6. **Psychometric Tests**: Scores on standardized psychometric tests, such as SAT or GRE scores, are often assumed to follow a normal distribution.\n",
    "\n",
    "The parameters of the normal distribution, namely the mean (μ) and standard deviation (σ), determine the shape, center, and spread of the distribution:\n",
    "\n",
    "1. **Mean (μ)**: The mean determines the center of the distribution. It represents the average value around which the data is centered. Shifting the mean to the left or right will move the peak of the distribution accordingly.\n",
    "\n",
    "2. **Standard Deviation (σ)**: The standard deviation determines the spread or width of the distribution. A larger standard deviation means the data points are more spread out from the mean, resulting in a wider distribution. Conversely, a smaller standard deviation results in a narrower distribution with data points clustered closely around the mean.\n",
    "\n",
    "In summary, the normal distribution is used to model data in situations where observations tend to cluster around a central value with symmetrical tails. The mean and standard deviation of the normal distribution control its center and spread, respectively, making it a versatile tool for modeling a wide range of phenomena in various fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q4: Explain the importance of Normal Distribution. Give a few real-life examples of Normal Distribution.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normal distribution, also known as the Gaussian distribution, holds immense importance in various fields due to its numerous desirable properties and wide applicability. Here are some reasons why the normal distribution is important:\n",
    "\n",
    "1. **Central Limit Theorem (CLT)**: One of the most fundamental properties of the normal distribution is its association with the Central Limit Theorem. According to the CLT, the sum (or average) of a large number of independent and identically distributed random variables, regardless of their original distribution, tends to follow a normal distribution. This property is crucial in statistical inference, hypothesis testing, and estimation.\n",
    "\n",
    "2. **Statistical Analysis**: Many statistical methods and techniques, such as hypothesis testing, confidence intervals, and regression analysis, rely on assumptions of normality. When data follow a normal distribution, these methods tend to be more robust and accurate.\n",
    "\n",
    "3. **Simplicity and Predictability**: The normal distribution is well-understood and characterized by its symmetrical bell-shaped curve. Its simplicity makes it easier to interpret and predict outcomes in various scenarios.\n",
    "\n",
    "4. **Modeling Uncertainty**: In many real-world situations, uncertainties and variations are inherent. The normal distribution provides a convenient and often realistic model for these uncertainties, allowing for better decision-making and risk management.\n",
    "\n",
    "5. **Approximation**: The normal distribution is often used as an approximation for other distributions, especially in cases where the exact distribution is unknown or difficult to work with. This approximation simplifies calculations and analysis.\n",
    "\n",
    "Real-life examples of situations where the normal distribution is commonly observed include:\n",
    "\n",
    "1. **Height of Individuals**: Human heights in a population often follow a normal distribution, with most people clustered around the average height and fewer individuals at the extremes (very tall or very short).\n",
    "\n",
    "2. **IQ Scores**: Intelligence quotient (IQ) scores tend to approximate a normal distribution in a population, with the majority of individuals clustered around the average IQ score and fewer individuals at the extremes of high or low IQ.\n",
    "\n",
    "3. **Measurement Errors**: Measurement errors in experiments or observations often follow a normal distribution, assuming random errors are present. This is commonly observed in fields such as physics, chemistry, and engineering.\n",
    "\n",
    "4. **Financial Data**: Returns on investments, such as stock prices, often exhibit a distribution that approximates a normal distribution under certain conditions, especially in the long run and when considering large portfolios.\n",
    "\n",
    "5. **Biological Phenomena**: Biological variables like birth weights, blood pressure, and enzyme activity levels in biological systems can sometimes be modeled using the normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q5: What is Bernaulli Distribution? Give an Example. What is the difference between Bernoulli Distribution and Binomial Distribution?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bernoulli distribution is a discrete probability distribution representing the outcomes of a single binary experiment, where the outcome can be either a success (usually denoted as 1) or a failure (usually denoted as 0), with fixed probabilities of success and failure. It's named after the Swiss mathematician Jacob Bernoulli.\n",
    "\n",
    "### Bernoulli Distribution:\n",
    "- **Parameters**: The Bernoulli distribution has one parameter, \\( p \\), which represents the probability of success in a single trial.\n",
    "- **Probability Mass Function (PMF)**:\n",
    "  \\[\n",
    "  P(X = x) = \n",
    "  \\begin{cases} \n",
    "  p & \\text{if } x = 1 \\\\\n",
    "  1 - p & \\text{if } x = 0 \\\\\n",
    "  0 & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "  \\]\n",
    "- **Example**: Consider flipping a fair coin, where getting a head is considered a success (1) and getting a tail is considered a failure (0). If the probability of getting a head is \\( p = 0.5 \\), then the outcome of each flip follows a Bernoulli distribution.\n",
    "\n",
    "### Binomial Distribution:\n",
    "The Binomial distribution represents the number of successes (or failures) in a fixed number of independent Bernoulli trials.\n",
    "\n",
    "- **Parameters**: The Binomial distribution has two parameters:\n",
    "  - \\( n \\): The number of independent Bernoulli trials.\n",
    "  - \\( p \\): The probability of success in each trial.\n",
    "- **Probability Mass Function (PMF)**:\n",
    "  \\[\n",
    "  P(X = k) = \\binom{n}{k} \\cdot p^k \\cdot (1 - p)^{n - k}\n",
    "  \\]\n",
    "  Where \\( k \\) is the number of successes.\n",
    "- **Example**: Suppose you flip a fair coin 10 times. The number of heads obtained follows a binomial distribution with parameters \\( n = 10 \\) (number of trials) and \\( p = 0.5 \\) (probability of success in each trial).\n",
    "\n",
    "### Difference between Bernoulli and Binomial Distribution:\n",
    "1. **Number of Trials**:\n",
    "   - Bernoulli Distribution: Represents a single trial.\n",
    "   - Binomial Distribution: Represents the number of successes in a fixed number of trials.\n",
    "\n",
    "2. **Parameters**:\n",
    "   - Bernoulli Distribution: Has one parameter, \\( p \\), representing the probability of success in a single trial.\n",
    "   - Binomial Distribution: Has two parameters, \\( n \\) and \\( p \\), representing the number of trials and the probability of success in each trial, respectively.\n",
    "\n",
    "3. **Outcome**:\n",
    "   - Bernoulli Distribution: The outcome of a single trial (0 or 1).\n",
    "   - Binomial Distribution: The number of successes in a fixed number of trials (0, 1, 2, ..., \\( n \\))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q6. Consider a dataset with a mean of 50 and a standard deviation of 10. If we assume that the dataset is normally distributed, what is the probability that a randomly selected observation will be greater than 60? Use the appropriate formula and show your calculations.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the probability that a randomly selected observation from a normally distributed dataset with a mean of 50 and a standard deviation of 10 will be greater than 60, we need to use the standard normal distribution (Z-distribution) and the properties of the normal distribution.\n",
    "\n",
    "1. **Standardize the value 60**:\n",
    "   We first standardize the value 60 using the formula for z-score:\n",
    "   \\[\n",
    "   Z = \\frac{X - \\mu}{\\sigma}\n",
    "   \\]\n",
    "   where \\( X \\) is the value (60 in this case), \\( \\mu \\) is the mean (50), and \\( \\sigma \\) is the standard deviation (10).\n",
    "   \n",
    "   Substituting the values:\n",
    "   \\[\n",
    "   Z = \\frac{60 - 50}{10} = \\frac{10}{10} = 1\n",
    "   \\]\n",
    "\n",
    "2. **Find the probability using the standard normal distribution table**:\n",
    "   We look up the probability corresponding to \\( Z = 1 \\) in the standard normal distribution table, which gives us the probability of the observation being less than 60. Since we want the probability of the observation being greater than 60, we subtract this probability from 1.\n",
    "   \n",
    "   From the standard normal distribution table, the probability corresponding to \\( Z = 1 \\) is approximately 0.8413.\n",
    "   \n",
    "3. **Calculate the probability**:\n",
    "   The probability of the observation being greater than 60 is:\n",
    "   \\[\n",
    "   P(X > 60) = 1 - P(X \\leq 60) = 1 - 0.8413 = 0.1587\n",
    "   \\]\n",
    "\n",
    "So, the probability that a randomly selected observation from the dataset will be greater than 60 is approximately 0.1587 or 15.87%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q7: Explain uniform Distribution with an example.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The uniform distribution is a continuous probability distribution where all outcomes in a given interval are equally likely. In other words, each value within the interval has the same probability of occurring. \n",
    "\n",
    "### Characteristics of the Uniform Distribution:\n",
    "1. **Constant Probability Density**: The probability density function (PDF) of the uniform distribution is constant within the interval and zero outside the interval.\n",
    "\n",
    "2. **Rectangular Shape**: The graph of the PDF of the uniform distribution forms a rectangle, reflecting the equal likelihood of outcomes within the interval.\n",
    "\n",
    "3. **Defined by Parameters**: The uniform distribution is defined by two parameters: \\(a\\) and \\(b\\), representing the lower and upper bounds of the interval, respectively.\n",
    "\n",
    "### Probability Density Function (PDF):\n",
    "The PDF of the uniform distribution is defined as:\n",
    "\\[ \n",
    "f(x|a,b) = \n",
    "\\begin{cases} \n",
    "\\frac{1}{b - a} & \\text{if } a \\leq x \\leq b \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "### Example:\n",
    "Consider the example of rolling a fair six-sided die. Each outcome (1, 2, 3, 4, 5, or 6) has an equal probability of \\( \\frac{1}{6} \\). This scenario follows a discrete uniform distribution.\n",
    "\n",
    "Another example is selecting a random number between 0 and 1 using a fair coin-tossing procedure. Any number within the interval [0, 1] has an equal chance of being selected. This illustrates a continuous uniform distribution.\n",
    "\n",
    "### Application:\n",
    "- **Random Number Generation**: The uniform distribution is commonly used in simulations and random number generation algorithms, where a random variable with equal likelihood of occurring within a certain range is required.\n",
    "- **Probability Models**: It serves as a simple baseline distribution for comparing more complex distributions or modeling situations where outcomes are equally likely over a continuous interval.\n",
    "\n",
    "In summary, the uniform distribution is characterized by its constant probability density within a defined interval, making it a fundamental concept in probability theory and statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q8: What is the z score? State the importance of the z score.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The z-score, also known as the standard score, is a measure of how many standard deviations a particular data point is away from the mean of a dataset. It's a dimensionless quantity that allows us to standardize and compare values from different distributions.\n",
    "\n",
    "### Formula for Calculating the z-score:\n",
    "The z-score of a data point \\( X \\) in a dataset with mean \\( \\mu \\) and standard deviation \\( \\sigma \\) is calculated using the formula:\n",
    "\\[ z = \\frac{X - \\mu}{\\sigma} \\]\n",
    "\n",
    "### Importance of the z-score:\n",
    "1. **Standardization and Comparison**: The z-score standardizes data, allowing for comparisons between data points from different distributions. It enables us to determine how far a data point deviates from the mean in terms of standard deviation units.\n",
    "\n",
    "2. **Identification of Outliers**: Extreme values (outliers) in a dataset can be easily identified using z-scores. Data points with z-scores significantly greater than or less than 0 (typically beyond ±2 or ±3) are considered outliers.\n",
    "\n",
    "3. **Probability Calculation**: The z-score is used in calculating probabilities under the normal distribution. It allows us to find the probability of a data point falling within a certain range or exceeding a specific value.\n",
    "\n",
    "4. **Hypothesis Testing**: In hypothesis testing, the z-score is used to determine the likelihood of observing a particular result under the null hypothesis. It helps in assessing the significance of experimental results.\n",
    "\n",
    "5. **Standardizing Variables**: In multivariate analysis, variables with different scales can be standardized using z-scores, allowing for fair comparison and interpretation of their effects.\n",
    "\n",
    "6. **Quality Control and Process Improvement**: In manufacturing and quality control processes, z-scores are used to monitor deviations from expected performance and identify areas for improvement.\n",
    "\n",
    "Overall, the z-score is a valuable statistical tool that provides insights into the relative position of data points within a distribution, aids in probability calculations, and supports various analytical and decision-making processes in fields ranging from finance and engineering to healthcare and social sciences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q9: What is Central Limit Theorem? State the significance of the Central Limit Theorem.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Central Limit Theorem (CLT) is a fundamental concept in probability theory and statistics. It states that the sum (or average) of a large number of independent and identically distributed random variables, regardless of their original distribution, tends to follow a normal distribution, as the sample size approaches infinity.\n",
    "\n",
    "### Statement of the Central Limit Theorem:\n",
    "Let \\( X_1, X_2, ..., X_n \\) be a sequence of independent and identically distributed random variables with mean \\( \\mu \\) and standard deviation \\( \\sigma \\). Then, as \\( n \\), the sample size, approaches infinity, the distribution of the sample mean \\( \\bar{X} \\) approaches a normal distribution with mean \\( \\mu \\) and standard deviation \\( \\frac{\\sigma}{\\sqrt{n}} \\).\n",
    "\n",
    "### Significance of the Central Limit Theorem:\n",
    "1. **Approximation of Distributions**: The CLT allows us to approximate the distribution of the sample mean (or sum) even when the original population distribution is not normal. This is incredibly useful because many real-world phenomena exhibit complex distributions that may not be easily characterized.\n",
    "\n",
    "2. **Statistical Inference**: The CLT forms the basis for many statistical methods and techniques, such as hypothesis testing, confidence intervals, and regression analysis. These methods rely on assumptions of normality or apply asymptotically, making the CLT indispensable for statistical inference.\n",
    "\n",
    "3. **Generalizability**: The CLT holds regardless of the underlying distribution of the individual random variables, as long as certain conditions are met (independence, identical distribution, finite variance). This makes it applicable to a wide range of scenarios and ensures the generalizability of statistical results.\n",
    "\n",
    "4. **Sampling Theory**: The CLT provides insights into the behavior of sample statistics, such as the sample mean and sample proportion, as sample size increases. It helps in understanding the stability and variability of sample estimates and guides the selection of appropriate sample sizes for reliable inference.\n",
    "\n",
    "5. **Quality Control and Process Improvement**: In fields such as manufacturing and quality control, the CLT is used to model the behavior of process parameters and to establish control limits for monitoring and improving processes.\n",
    "\n",
    "In summary, the Central Limit Theorem is a cornerstone of statistical theory with profound implications for data analysis, inference, and decision-making in various disciplines. It enables us to make reliable statistical inferences even when dealing with complex and diverse datasets, making it one of the most powerful tools in the statistical toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q10: State the assumptions of the Central Limit Theorem.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Central Limit Theorem (CLT) is a powerful statistical concept, but it relies on certain assumptions to hold true. Here are the key assumptions of the Central Limit Theorem:\n",
    "\n",
    "1. **Independence**: The random variables in the sample must be independent of each other. This means that the outcome of one observation does not influence the outcome of another observation.\n",
    "\n",
    "2. **Identical Distribution**: The random variables in the sample must be identically distributed. This means that they come from the same population or have the same probability distribution function.\n",
    "\n",
    "3. **Finite Variance**: The population from which the sample is drawn must have a finite variance (\\( \\sigma^2 \\)). In practical terms, this means that the spread of the population is not infinite.\n",
    "\n",
    "4. **Random Sampling**: The sample must be obtained through random sampling, where each member of the population has an equal chance of being selected. This ensures that the sample is representative of the population.\n",
    "\n",
    "5. **Sample Size**: The CLT holds better as the sample size (\\( n \\)) increases. While there is no strict rule for how large \\( n \\) must be, a commonly cited guideline is that \\( n \\) should be at least 30. However, in some cases, even smaller sample sizes can lead to approximately normal distributions if the underlying population distribution is not heavily skewed.\n",
    "\n",
    "These assumptions are crucial for the CLT to be applicable and for the sample mean or sum to approximate a normal distribution. Violation of these assumptions may lead to inaccurate results or unreliable inference when using the CLT. Therefore, it's essential to assess whether these assumptions are met before applying the CLT in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **COMPLETE**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
