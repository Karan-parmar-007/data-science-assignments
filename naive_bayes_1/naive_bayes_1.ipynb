{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Na√Øve bayes-1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure, let's go through these questions one by one.\n",
    "\n",
    "### Q1. What is Bayes' theorem?\n",
    "\n",
    "**Answer:**\n",
    "Bayes' theorem is a fundamental principle in probability theory and statistics that describes the probability of an event based on prior knowledge of conditions that might be related to the event. It allows for the updating of the probability estimate for an event as more evidence or information becomes available.\n",
    "\n",
    "### Q2. What is the formula for Bayes' theorem?\n",
    "\n",
    "**Answer:**\n",
    "The formula for Bayes' theorem is:\n",
    "\n",
    "\\[ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} \\]\n",
    "\n",
    "where:\n",
    "- \\( P(A|B) \\) is the posterior probability of event \\( A \\) given that event \\( B \\) has occurred.\n",
    "- \\( P(B|A) \\) is the likelihood of event \\( B \\) given that event \\( A \\) has occurred.\n",
    "- \\( P(A) \\) is the prior probability of event \\( A \\).\n",
    "- \\( P(B) \\) is the marginal probability of event \\( B \\).\n",
    "\n",
    "### Q3. How is Bayes' theorem used in practice?\n",
    "\n",
    "**Answer:**\n",
    "Bayes' theorem is used in various fields such as:\n",
    "\n",
    "- **Medical Diagnosis:** To update the probability of a disease given new test results.\n",
    "- **Spam Filtering:** To determine the probability that an email is spam based on the presence of certain words.\n",
    "- **Machine Learning:** Particularly in classification problems using Naive Bayes classifiers.\n",
    "- **Finance:** To update the probability of market trends or economic events.\n",
    "- **Forensics:** To update the likelihood of a suspect's involvement in a crime based on new evidence.\n",
    "\n",
    "### Q4. What is the relationship between Bayes' theorem and conditional probability?\n",
    "\n",
    "**Answer:**\n",
    "Bayes' theorem is fundamentally based on the concept of conditional probability. It provides a way to invert conditional probabilities. While conditional probability \\( P(A|B) \\) tells us the probability of \\( A \\) happening given that \\( B \\) has happened, Bayes' theorem allows us to compute this in terms of the reverse conditional probability \\( P(B|A) \\) and the marginal probabilities \\( P(A) \\) and \\( P(B) \\).\n",
    "\n",
    "### Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?\n",
    "\n",
    "**Answer:**\n",
    "The choice of Naive Bayes classifier depends on the nature of the data:\n",
    "\n",
    "- **Gaussian Naive Bayes:** Use this when the features are continuous and you assume they follow a Gaussian (normal) distribution.\n",
    "- **Multinomial Naive Bayes:** This is suitable for discrete data, particularly for text classification problems such as spam detection, where word counts or frequencies are used as features.\n",
    "- **Bernoulli Naive Bayes:** This is ideal for binary/Boolean features, where features are binary (0 or 1), such as in binary text classification where we track whether a word occurs in a document or not.\n",
    "\n",
    "### Q6. Assignment:\n",
    "You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of each feature value for each class:\n",
    "\n",
    "| Class | X1=1 | X1=2 | X1=3 | X2=1 | X2=2 | X2=3 | X2=4 |\n",
    "|-------|------|------|------|------|------|------|------|\n",
    "| A     | 3    | 3    | 4    | 4    | 3    | 3    | 3    |\n",
    "| B     | 2    | 2    | 1    | 2    | 2    | 2    | 3    |\n",
    "\n",
    "Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance to belong to?\n",
    "\n",
    "**Answer:**\n",
    "First, we calculate the probabilities for each class based on the given frequencies. The prior probabilities are equal, so \\( P(A) = P(B) = 0.5 \\).\n",
    "\n",
    "Calculate the likelihoods for each feature given the class:\n",
    "- \\( P(X1=3|A) = \\frac{\\text{count of } X1=3 \\text{ in class A}}{\\text{total count of class A}} = \\frac{4}{19} \\)\n",
    "- \\( P(X2=4|A) = \\frac{\\text{count of } X2=4 \\text{ in class A}}{\\text{total count of class A}} = \\frac{3}{19} \\)\n",
    "\n",
    "- \\( P(X1=3|B) = \\frac{\\text{count of } X1=3 \\text{ in class B}}{\\text{total count of class B}} = \\frac{1}{10} \\)\n",
    "- \\( P(X2=4|B) = \\frac{\\text{count of } X2=4 \\text{ in class B}}{\\text{total count of class B}} = \\frac{3}{10} \\)\n",
    "\n",
    "Calculate the posterior probabilities for each class:\n",
    "- For class A: \\( P(A|X1=3, X2=4) \\propto P(X1=3|A) \\cdot P(X2=4|A) \\cdot P(A) \\)\n",
    "- For class B: \\( P(B|X1=3, X2=4) \\propto P(X1=3|B) \\cdot P(X2=4|B) \\cdot P(B) \\)\n",
    "\n",
    "Plugging in the values:\n",
    "- \\( P(A|X1=3, X2=4) \\propto \\frac{4}{19} \\cdot \\frac{3}{19} \\cdot 0.5 \\)\n",
    "- \\( P(B|X1=3, X2=4) \\propto \\frac{1}{10} \\cdot \\frac{3}{10} \\cdot 0.5 \\)\n",
    "\n",
    "Compare the two:\n",
    "- \\( P(A|X1=3, X2=4) \\propto \\frac{12}{361} \\cdot 0.5 = \\frac{6}{361} \\)\n",
    "- \\( P(B|X1=3, X2=4) \\propto \\frac{3}{100} \\cdot 0.5 = \\frac{1.5}{100} = \\frac{15}{1000} = \\frac{3}{200} = \\frac{5.415}{361} \\)\n",
    "\n",
    "Since \\(\\frac{6}{361}\\) is greater than \\(\\frac{5.415}{361}\\), the classifier predicts the new instance to belong to class A."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
